#!/usr/bin/env python3
"""
æ•°æ®é›†éªŒè¯å’Œç»Ÿè®¡è„šæœ¬

éªŒè¯æ•°æ®é›†çš„å®Œæ•´æ€§ã€ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/validate_data.py --dataset cats_and_dogs
    python scripts/validate_data.py --all
    python scripts/validate_data.py --dataset cats_and_dogs --detailed
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter
import time
from PIL import Image
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class DatasetValidator:
    """æ•°æ®é›†éªŒè¯å™¨"""
    
    def __init__(self, data_root: str = "data"):
        self.data_root = Path(data_root)
        self.supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        
    def validate_dataset(self, dataset_name: str, detailed: bool = False) -> Dict:
        """
        éªŒè¯æŒ‡å®šæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            detailed: æ˜¯å¦è¿›è¡Œè¯¦ç»†éªŒè¯
            
        Returns:
            Dict: éªŒè¯ç»“æœ
        """
        dataset_path = self.data_root / dataset_name
        
        if not dataset_path.exists():
            return {
                "dataset": dataset_name,
                "status": "error",
                "message": f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_path}"
            }
        
        print(f"ğŸ” éªŒè¯æ•°æ®é›†: {dataset_name}")
        print(f"   è·¯å¾„: {dataset_path}")
        
        result = {
            "dataset": dataset_name,
            "path": str(dataset_path),
            "status": "success",
            "timestamp": time.time(),
            "structure": {},
            "statistics": {},
            "issues": []
        }
        
        try:
            # åŸºæœ¬ç»“æ„éªŒè¯
            result["structure"] = self._validate_structure(dataset_path)
            
            # ç»Ÿè®¡ä¿¡æ¯
            result["statistics"] = self._collect_statistics(dataset_path)
            
            # è¯¦ç»†éªŒè¯
            if detailed:
                result["detailed"] = self._detailed_validation(dataset_path)
            
            # æ£€æŸ¥é—®é¢˜
            result["issues"] = self._check_issues(dataset_path, result["statistics"])
            
            # ç¡®å®šæ•´ä½“çŠ¶æ€
            if result["issues"]:
                result["status"] = "warning" if any(issue["level"] == "error" for issue in result["issues"]) else "warning"
            
            self._print_validation_report(result)
            
        except Exception as e:
            result["status"] = "error"
            result["message"] = str(e)
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        
        return result
    
    def _validate_structure(self, dataset_path: Path) -> Dict:
        """éªŒè¯æ•°æ®é›†ç›®å½•ç»“æ„"""
        structure = {
            "has_train": False,
            "has_val": False,
            "has_test": False,
            "train_classes": [],
            "val_classes": [],
            "test_classes": [],
            "readme_exists": False
        }
        
        # æ£€æŸ¥ä¸»è¦ç›®å½•
        train_path = dataset_path / "train"
        val_path = dataset_path / "val"
        test_path = dataset_path / "test"
        readme_path = dataset_path / "README.md"
        
        structure["has_train"] = train_path.exists() and train_path.is_dir()
        structure["has_val"] = val_path.exists() and val_path.is_dir()
        structure["has_test"] = test_path.exists() and test_path.is_dir()
        structure["readme_exists"] = readme_path.exists()
        
        # æ£€æŸ¥ç±»åˆ«ç›®å½•
        if structure["has_train"]:
            structure["train_classes"] = [d.name for d in train_path.iterdir() if d.is_dir()]
        
        if structure["has_val"]:
            structure["val_classes"] = [d.name for d in val_path.iterdir() if d.is_dir()]
        
        if structure["has_test"]:
            structure["test_classes"] = [d.name for d in test_path.iterdir() if d.is_dir()]
        
        return structure
    
    def _collect_statistics(self, dataset_path: Path) -> Dict:
        """æ”¶é›†æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_files": 0,
            "total_size_mb": 0,
            "file_formats": Counter(),
            "splits": {},
            "class_distribution": {}
        }
        
        for split in ["train", "val", "test"]:
            split_path = dataset_path / split
            if split_path.exists():
                split_stats = self._analyze_split(split_path)
                stats["splits"][split] = split_stats
                stats["total_files"] += split_stats["total_files"]
                stats["total_size_mb"] += split_stats["total_size_mb"]
                
                # åˆå¹¶æ–‡ä»¶æ ¼å¼ç»Ÿè®¡
                for fmt, count in split_stats["file_formats"].items():
                    stats["file_formats"][fmt] += count
                
                # åˆå¹¶ç±»åˆ«åˆ†å¸ƒ
                for cls, count in split_stats["class_distribution"].items():
                    if cls not in stats["class_distribution"]:
                        stats["class_distribution"][cls] = {}
                    stats["class_distribution"][cls][split] = count
        
        return stats
    
    def _analyze_split(self, split_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ•°æ®åˆ†å‰²"""
        stats = {
            "total_files": 0,
            "total_size_mb": 0,
            "file_formats": Counter(),
            "class_distribution": {},
            "avg_file_size_kb": 0,
            "image_dimensions": []
        }
        
        total_size_bytes = 0
        
        for class_dir in split_path.iterdir():
            if not class_dir.is_dir():
                continue
            
            class_name = class_dir.name
            class_files = []
            
            for file_path in class_dir.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in self.supported_formats:
                    class_files.append(file_path)
                    file_size = file_path.stat().st_size
                    total_size_bytes += file_size
                    
                    # ç»Ÿè®¡æ–‡ä»¶æ ¼å¼
                    stats["file_formats"][file_path.suffix.lower()] += 1
            
            stats["class_distribution"][class_name] = len(class_files)
            stats["total_files"] += len(class_files)
        
        stats["total_size_mb"] = total_size_bytes / (1024 * 1024)
        if stats["total_files"] > 0:
            stats["avg_file_size_kb"] = total_size_bytes / stats["total_files"] / 1024
        
        return stats
    
    def _detailed_validation(self, dataset_path: Path) -> Dict:
        """è¯¦ç»†éªŒè¯ï¼ˆæ£€æŸ¥å›¾ç‰‡å®Œæ•´æ€§ç­‰ï¼‰"""
        detailed = {
            "corrupted_files": [],
            "duplicate_files": [],
            "unusual_dimensions": [],
            "file_integrity": {"passed": 0, "failed": 0}
        }
        
        print("   ğŸ” è¿›è¡Œè¯¦ç»†éªŒè¯...")
        
        file_hashes = {}
        
        for split in ["train", "val", "test"]:
            split_path = dataset_path / split
            if not split_path.exists():
                continue
            
            for class_dir in split_path.iterdir():
                if not class_dir.is_dir():
                    continue
                
                for file_path in class_dir.iterdir():
                    if file_path.suffix.lower() not in self.supported_formats:
                        continue
                    
                    try:
                        # æ£€æŸ¥å›¾ç‰‡å®Œæ•´æ€§
                        with Image.open(file_path) as img:
                            img.verify()  # éªŒè¯å›¾ç‰‡
                            
                            # é‡æ–°æ‰“å¼€è·å–å°ºå¯¸ä¿¡æ¯
                            with Image.open(file_path) as img2:
                                width, height = img2.size
                                
                                # æ£€æŸ¥å¼‚å¸¸å°ºå¯¸
                                if width < 32 or height < 32 or width > 5000 or height > 5000:
                                    detailed["unusual_dimensions"].append({
                                        "file": str(file_path.relative_to(dataset_path)),
                                        "dimensions": f"{width}x{height}"
                                    })
                        
                        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œæ£€æŸ¥é‡å¤
                        file_hash = self._calculate_file_hash(file_path)
                        if file_hash in file_hashes:
                            detailed["duplicate_files"].append({
                                "file1": str(file_hashes[file_hash].relative_to(dataset_path)),
                                "file2": str(file_path.relative_to(dataset_path))
                            })
                        else:
                            file_hashes[file_hash] = file_path
                        
                        detailed["file_integrity"]["passed"] += 1
                        
                    except Exception as e:
                        detailed["corrupted_files"].append({
                            "file": str(file_path.relative_to(dataset_path)),
                            "error": str(e)
                        })
                        detailed["file_integrity"]["failed"] += 1
        
        return detailed
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _check_issues(self, dataset_path: Path, statistics: Dict) -> List[Dict]:
        """æ£€æŸ¥æ•°æ®é›†é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒé›†
        if "train" not in statistics["splits"]:
            issues.append({
                "level": "error",
                "type": "missing_split",
                "message": "ç¼ºå°‘è®­ç»ƒé›†ç›®å½•"
            })
        
        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é›†
        if "val" not in statistics["splits"]:
            issues.append({
                "level": "warning",
                "type": "missing_split",
                "message": "ç¼ºå°‘éªŒè¯é›†ç›®å½•"
            })
        
        # æ£€æŸ¥ç±»åˆ«ä¸€è‡´æ€§
        if "train" in statistics["splits"] and "val" in statistics["splits"]:
            train_classes = set(statistics["class_distribution"].keys())
            val_classes = set()
            for cls, splits in statistics["class_distribution"].items():
                if "val" in splits:
                    val_classes.add(cls)
            
            if train_classes != val_classes:
                issues.append({
                    "level": "warning",
                    "type": "class_mismatch",
                    "message": f"è®­ç»ƒé›†å’ŒéªŒè¯é›†ç±»åˆ«ä¸ä¸€è‡´: è®­ç»ƒé›†{train_classes}, éªŒè¯é›†{val_classes}"
                })
        
        # æ£€æŸ¥ç±»åˆ«å¹³è¡¡
        for split, split_stats in statistics["splits"].items():
            class_counts = list(split_stats["class_distribution"].values())
            if len(class_counts) > 1:
                min_count = min(class_counts)
                max_count = max(class_counts)
                if max_count / min_count > 3:  # ä¸å¹³è¡¡é˜ˆå€¼
                    issues.append({
                        "level": "warning",
                        "type": "class_imbalance",
                        "message": f"{split}é›†ç±»åˆ«ä¸å¹³è¡¡: æœ€å°‘{min_count}ä¸ªæ ·æœ¬, æœ€å¤š{max_count}ä¸ªæ ·æœ¬"
                    })
        
        # æ£€æŸ¥æ–‡ä»¶æ•°é‡
        if statistics["total_files"] < 100:
            issues.append({
                "level": "warning",
                "type": "small_dataset",
                "message": f"æ•°æ®é›†è¾ƒå°ï¼Œä»…æœ‰{statistics['total_files']}ä¸ªæ–‡ä»¶"
            })
        
        return issues
    
    def _print_validation_report(self, result: Dict) -> None:
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print(f"\nğŸ“Š éªŒè¯æŠ¥å‘Š: {result['dataset']}")
        print("=" * 50)
        
        # çŠ¶æ€
        status_emoji = {"success": "âœ…", "warning": "âš ï¸", "error": "âŒ"}
        print(f"çŠ¶æ€: {status_emoji.get(result['status'], 'â“')} {result['status'].upper()}")
        
        # ç»“æ„ä¿¡æ¯
        structure = result["structure"]
        print(f"\nğŸ“ ç›®å½•ç»“æ„:")
        print(f"   è®­ç»ƒé›†: {'âœ…' if structure['has_train'] else 'âŒ'}")
        print(f"   éªŒè¯é›†: {'âœ…' if structure['has_val'] else 'âŒ'}")
        print(f"   æµ‹è¯•é›†: {'âœ…' if structure['has_test'] else 'âŒ'}")
        print(f"   README: {'âœ…' if structure['readme_exists'] else 'âŒ'}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = result["statistics"]
        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»æ–‡ä»¶æ•°: {stats['total_files']:,}")
        print(f"   æ€»å¤§å°: {stats['total_size_mb']:.1f} MB")
        print(f"   æ–‡ä»¶æ ¼å¼: {dict(stats['file_formats'])}")
        
        # åˆ†å‰²ç»Ÿè®¡
        for split, split_stats in stats["splits"].items():
            print(f"\n   {split.upper()}é›†:")
            print(f"     æ–‡ä»¶æ•°: {split_stats['total_files']:,}")
            print(f"     å¤§å°: {split_stats['total_size_mb']:.1f} MB")
            print(f"     å¹³å‡æ–‡ä»¶å¤§å°: {split_stats['avg_file_size_kb']:.1f} KB")
            print(f"     ç±»åˆ«åˆ†å¸ƒ: {split_stats['class_distribution']}")
        
        # é—®é¢˜æŠ¥å‘Š
        if result["issues"]:
            print(f"\nâš ï¸ å‘ç°é—®é¢˜:")
            for issue in result["issues"]:
                level_emoji = {"error": "âŒ", "warning": "âš ï¸", "info": "â„¹ï¸"}
                print(f"   {level_emoji.get(issue['level'], 'â“')} {issue['message']}")
        else:
            print(f"\nâœ… æœªå‘ç°é—®é¢˜")
        
        # è¯¦ç»†éªŒè¯ç»“æœ
        if "detailed" in result:
            detailed = result["detailed"]
            print(f"\nğŸ” è¯¦ç»†éªŒè¯:")
            print(f"   æ–‡ä»¶å®Œæ•´æ€§: {detailed['file_integrity']['passed']} é€šè¿‡, {detailed['file_integrity']['failed']} å¤±è´¥")
            
            if detailed["corrupted_files"]:
                print(f"   æŸåæ–‡ä»¶: {len(detailed['corrupted_files'])} ä¸ª")
                for corrupt in detailed["corrupted_files"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"     - {corrupt['file']}: {corrupt['error']}")
            
            if detailed["duplicate_files"]:
                print(f"   é‡å¤æ–‡ä»¶: {len(detailed['duplicate_files'])} å¯¹")
                for dup in detailed["duplicate_files"][:3]:  # åªæ˜¾ç¤ºå‰3å¯¹
                    print(f"     - {dup['file1']} â‰ˆ {dup['file2']}")
            
            if detailed["unusual_dimensions"]:
                print(f"   å¼‚å¸¸å°ºå¯¸: {len(detailed['unusual_dimensions'])} ä¸ª")
                for unusual in detailed["unusual_dimensions"][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"     - {unusual['file']}: {unusual['dimensions']}")
    
    def validate_all_datasets(self, detailed: bool = False) -> Dict[str, Dict]:
        """éªŒè¯æ‰€æœ‰æ•°æ®é›†"""
        results = {}
        
        if not self.data_root.exists():
            print(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {self.data_root}")
            return results
        
        datasets = [d.name for d in self.data_root.iterdir() if d.is_dir()]
        
        if not datasets:
            print(f"â„¹ï¸ æ•°æ®æ ¹ç›®å½•ä¸ºç©º: {self.data_root}")
            return results
        
        print(f"ğŸ” éªŒè¯æ‰€æœ‰æ•°æ®é›† (å…± {len(datasets)} ä¸ª)")
        print("=" * 60)
        
        for dataset in datasets:
            print(f"\n{'='*20} {dataset} {'='*20}")
            results[dataset] = self.validate_dataset(dataset, detailed)
        
        # æ€»ç»“æŠ¥å‘Š
        print(f"\nğŸ“‹ æ€»ç»“æŠ¥å‘Š")
        print("=" * 60)
        
        success_count = sum(1 for r in results.values() if r["status"] == "success")
        warning_count = sum(1 for r in results.values() if r["status"] == "warning")
        error_count = sum(1 for r in results.values() if r["status"] == "error")
        
        print(f"âœ… æˆåŠŸ: {success_count}")
        print(f"âš ï¸ è­¦å‘Š: {warning_count}")
        print(f"âŒ é”™è¯¯: {error_count}")
        
        return results
    
    def save_report(self, results: Dict, output_path: str = "data_validation_report.json") -> None:
        """ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®é›†éªŒè¯å·¥å…·")
    parser.add_argument("--dataset", type=str, help="è¦éªŒè¯çš„æ•°æ®é›†åç§°")
    parser.add_argument("--all", action="store_true", help="éªŒè¯æ‰€æœ‰æ•°æ®é›†")
    parser.add_argument("--detailed", action="store_true", help="è¿›è¡Œè¯¦ç»†éªŒè¯")
    parser.add_argument("--data-root", type=str, default="data", help="æ•°æ®æ ¹ç›®å½•")
    parser.add_argument("--save-report", type=str, help="ä¿å­˜æŠ¥å‘Šåˆ°æŒ‡å®šæ–‡ä»¶")
    
    args = parser.parse_args()
    
    validator = DatasetValidator(args.data_root)
    
    if args.all:
        results = validator.validate_all_datasets(args.detailed)
    elif args.dataset:
        results = {args.dataset: validator.validate_dataset(args.dataset, args.detailed)}
    else:
        parser.print_help()
        return
    
    if args.save_report:
        validator.save_report(results, args.save_report)

if __name__ == "__main__":
    main()