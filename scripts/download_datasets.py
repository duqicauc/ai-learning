#!/usr/bin/env python3
"""
æ•°æ®é›†ä¸‹è½½å’Œç®¡ç†è„šæœ¬

æ”¯æŒä¸‹è½½å¸¸ç”¨çš„æœºå™¨å­¦ä¹ æ•°æ®é›†ï¼Œå¹¶è‡ªåŠ¨ç»„ç»‡ä¸ºæ ‡å‡†ç›®å½•ç»“æ„ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_datasets.py --dataset cats_and_dogs
    python scripts/download_datasets.py --dataset cifar10
    python scripts/download_datasets.py --list
"""

import os
import sys
import argparse
import urllib.request
import zipfile
import tarfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# æ•°æ®é›†é…ç½®
DATASETS_CONFIG = {
    "cats_and_dogs": {
        "name": "Cats and Dogs Classification",
        "description": "äºŒåˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«çŒ«å’Œç‹—çš„å›¾ç‰‡",
        "url": "https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip",
        "size": "~800MB",
        "classes": ["cat", "dog"],
        "format": "jpg",
        "structure": "PetImages/{Cat,Dog}/*.jpg",
        "preprocessing": "éœ€è¦æ¸…ç†æŸåçš„å›¾ç‰‡æ–‡ä»¶"
    },
    "fruits100": {
        "name": "Fruits 100 Classification",
        "description": "100ç§æ°´æœåˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„æ°´æœå›¾åƒ",
        "url": "https://www.modelscope.cn/datasets/tany0699/fruits100.git",
        "size": "~500MB",
        "classes": "100ç§æ°´æœç±»åˆ«",
        "format": "jpg",
        "structure": "train/{fruit_name}/*.jpg, val/{fruit_name}/*.jpg",
        "preprocessing": "ModelScopeæ•°æ®é›†ï¼Œéœ€è¦ä½¿ç”¨git cloneä¸‹è½½",
        "download_method": "git_clone"
    },
    "cifar10": {
        "name": "CIFAR-10",
        "description": "10ç±»ç‰©ä½“è¯†åˆ«æ•°æ®é›†",
        "url": "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz",
        "size": "~170MB",
        "classes": ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"],
        "format": "pickle",
        "structure": "cifar-10-batches-py/",
        "preprocessing": "éœ€è¦è½¬æ¢ä¸ºå›¾ç‰‡æ ¼å¼"
    },
    "mnist": {
        "name": "MNIST",
        "description": "æ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›†",
        "url": "http://yann.lecun.com/exdb/mnist/",
        "size": "~60MB",
        "classes": ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
        "format": "idx",
        "structure": "MNIST/raw/",
        "preprocessing": "PyTorchå¯ç›´æ¥åŠ è½½"
    }
}

class DatasetDownloader:
    """æ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, data_root: str = "data"):
        self.data_root = Path(data_root)
        self.data_root.mkdir(exist_ok=True)
        
    def list_datasets(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        print("ğŸ“Š å¯ç”¨æ•°æ®é›†åˆ—è¡¨:")
        print("=" * 60)
        
        for dataset_id, config in DATASETS_CONFIG.items():
            status = "âœ… å·²ä¸‹è½½" if self._is_downloaded(dataset_id) else "â¬‡ï¸ æœªä¸‹è½½"
            print(f"\nğŸ”¹ {dataset_id}")
            print(f"   åç§°: {config['name']}")
            print(f"   æè¿°: {config['description']}")
            print(f"   å¤§å°: {config['size']}")
            print(f"   ç±»åˆ«æ•°: {len(config['classes'])}")
            print(f"   çŠ¶æ€: {status}")
    
    def _is_downloaded(self, dataset_id: str) -> bool:
        """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ä¸‹è½½"""
        dataset_path = self.data_root / dataset_id
        return dataset_path.exists() and any(dataset_path.iterdir())
    
    def download_dataset(self, dataset_id: str, force: bool = False) -> bool:
        """
        ä¸‹è½½æŒ‡å®šæ•°æ®é›†
        
        Args:
            dataset_id: æ•°æ®é›†ID
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            
        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if dataset_id not in DATASETS_CONFIG:
            print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_id}")
            print(f"å¯ç”¨æ•°æ®é›†: {list(DATASETS_CONFIG.keys())}")
            return False
        
        config = DATASETS_CONFIG[dataset_id]
        dataset_path = self.data_root / dataset_id
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if self._is_downloaded(dataset_id) and not force:
            print(f"âœ… æ•°æ®é›† {dataset_id} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            print(f"   è·¯å¾„: {dataset_path}")
            print(f"   ä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°ä¸‹è½½")
            return True
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ•°æ®é›†: {config['name']}")
        print(f"   å¤§å°: {config['size']}")
        print(f"   URL: {config['url']}")
        
        try:
            # åˆ›å»ºæ•°æ®é›†ç›®å½•
            dataset_path.mkdir(exist_ok=True)
            
            # æ ¹æ®æ•°æ®é›†ç±»å‹è°ƒç”¨ç›¸åº”çš„ä¸‹è½½æ–¹æ³•
            if dataset_id == "cats_and_dogs":
                return self._download_cats_and_dogs(config, dataset_path)
            elif dataset_id == "cifar10":
                return self._download_cifar10(config, dataset_path)
            elif dataset_id == "mnist":
                return self._download_mnist(config, dataset_path)
            else:
                print(f"âŒ æš‚ä¸æ”¯æŒè‡ªåŠ¨ä¸‹è½½ {dataset_id}")
                return False
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def _download_with_progress(self, url: str, filepath: Path) -> None:
        """å¸¦è¿›åº¦æ¡çš„æ–‡ä»¶ä¸‹è½½"""
        def progress_hook(block_num, block_size, total_size):
            downloaded = block_num * block_size
            if total_size > 0:
                percent = min(100, downloaded * 100 / total_size)
                print(f"\r   ä¸‹è½½è¿›åº¦: {percent:.1f}% ({downloaded // 1024 // 1024}MB / {total_size // 1024 // 1024}MB)", end="")
        
        print(f"   æ­£åœ¨ä¸‹è½½åˆ°: {filepath}")
        urllib.request.urlretrieve(url, filepath, progress_hook)
        print()  # æ¢è¡Œ
    
    def _download_cats_and_dogs(self, config: Dict, dataset_path: Path) -> bool:
        """ä¸‹è½½çŒ«ç‹—æ•°æ®é›†"""
        zip_path = dataset_path / "kagglecatsanddogs.zip"
        
        # ä¸‹è½½ZIPæ–‡ä»¶
        self._download_with_progress(config["url"], zip_path)
        
        print("   æ­£åœ¨è§£å‹æ–‡ä»¶...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(dataset_path)
        
        # é‡ç»„ç›®å½•ç»“æ„
        print("   æ­£åœ¨é‡ç»„ç›®å½•ç»“æ„...")
        pet_images_path = dataset_path / "PetImages"
        
        if pet_images_path.exists():
            # åˆ›å»ºæ ‡å‡†ç›®å½•ç»“æ„
            train_path = dataset_path / "train"
            val_path = dataset_path / "val"
            train_path.mkdir(exist_ok=True)
            val_path.mkdir(exist_ok=True)
            
            for class_name in ["Cat", "Dog"]:
                class_path = pet_images_path / class_name
                if class_path.exists():
                    # åˆ›å»ºç±»åˆ«ç›®å½•
                    train_class_path = train_path / class_name.lower()
                    val_class_path = val_path / class_name.lower()
                    train_class_path.mkdir(exist_ok=True)
                    val_class_path.mkdir(exist_ok=True)
                    
                    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
                    image_files = list(class_path.glob("*.jpg"))
                    print(f"   å¤„ç† {class_name}: {len(image_files)} å¼ å›¾ç‰‡")
                    
                    # æ¸…ç†æŸåçš„æ–‡ä»¶
                    valid_files = []
                    for img_file in image_files:
                        try:
                            # ç®€å•æ£€æŸ¥æ–‡ä»¶å¤§å°
                            if img_file.stat().st_size > 1000:  # å¤§äº1KB
                                valid_files.append(img_file)
                        except:
                            continue
                    
                    print(f"   æœ‰æ•ˆå›¾ç‰‡: {len(valid_files)} å¼ ")
                    
                    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80:20)
                    split_idx = int(len(valid_files) * 0.8)
                    train_files = valid_files[:split_idx]
                    val_files = valid_files[split_idx:]
                    
                    # å¤åˆ¶æ–‡ä»¶
                    for i, img_file in enumerate(train_files):
                        new_name = f"{class_name.lower()}_{i+1:04d}.jpg"
                        shutil.copy2(img_file, train_class_path / new_name)
                    
                    for i, img_file in enumerate(val_files):
                        new_name = f"{class_name.lower()}_{i+1:04d}.jpg"
                        shutil.copy2(img_file, val_class_path / new_name)
            
            # æ¸…ç†åŸå§‹æ–‡ä»¶
            shutil.rmtree(pet_images_path)
        
        # åˆ é™¤ZIPæ–‡ä»¶
        zip_path.unlink()
        
        # åˆ›å»ºREADMEæ–‡ä»¶
        self._create_dataset_readme(dataset_path, config)
        
        print("âœ… çŒ«ç‹—æ•°æ®é›†ä¸‹è½½å®Œæˆ!")
        return True
    
    def _download_cifar10(self, config: Dict, dataset_path: Path) -> bool:
        """ä¸‹è½½CIFAR-10æ•°æ®é›†"""
        print("â„¹ï¸  CIFAR-10æ•°æ®é›†å»ºè®®ä½¿ç”¨PyTorchå†…ç½®ä¸‹è½½:")
        print("   from torchvision import datasets")
        print("   datasets.CIFAR10(root='data', train=True, download=True)")
        
        # è¿™é‡Œå¯ä»¥å®ç°è‡ªå®šä¹‰ä¸‹è½½é€»è¾‘
        return False
    
    def _download_mnist(self, config: Dict, dataset_path: Path) -> bool:
        """ä¸‹è½½MNISTæ•°æ®é›†"""
        print("â„¹ï¸  MNISTæ•°æ®é›†å»ºè®®ä½¿ç”¨PyTorchå†…ç½®ä¸‹è½½:")
        print("   from torchvision import datasets")
        print("   datasets.MNIST(root='data', train=True, download=True)")
        
        # è¿™é‡Œå¯ä»¥å®ç°è‡ªå®šä¹‰ä¸‹è½½é€»è¾‘
        return False
    
    def _create_dataset_readme(self, dataset_path: Path, config: Dict) -> None:
        """åˆ›å»ºæ•°æ®é›†READMEæ–‡ä»¶"""
        readme_content = f"""# {config['name']}

## æ•°æ®é›†ä¿¡æ¯
- **æè¿°**: {config['description']}
- **å¤§å°**: {config['size']}
- **ç±»åˆ«æ•°**: {len(config['classes'])}
- **æ ¼å¼**: {config['format']}

## ç±»åˆ«åˆ—è¡¨
{chr(10).join(f"- {cls}" for cls in config['classes'])}

## ç›®å½•ç»“æ„
```
{dataset_path.name}/
â”œâ”€â”€ train/          # è®­ç»ƒé›†
â”‚   â”œâ”€â”€ {config['classes'][0]}/
â”‚   â””â”€â”€ {config['classes'][1] if len(config['classes']) > 1 else '...'}/
â”œâ”€â”€ val/            # éªŒè¯é›†
â”‚   â”œâ”€â”€ {config['classes'][0]}/
â”‚   â””â”€â”€ {config['classes'][1] if len(config['classes']) > 1 else '...'}/
â””â”€â”€ README.md       # æœ¬æ–‡ä»¶
```

## ä½¿ç”¨æ–¹æ³•
```python
from src.utils.data_config import get_{dataset_path.name.replace('-', '_')}_paths

# è·å–æ•°æ®è·¯å¾„
train_dir, val_dir, test_dir = get_{dataset_path.name.replace('-', '_')}_paths()

# åŠ è½½æ•°æ®é›†
from torchvision import datasets, transforms
train_dataset = datasets.ImageFolder(train_dir, transform=transforms.ToTensor())
```

## æ•°æ®ç»Ÿè®¡
- è®­ç»ƒé›†: å¾…ç»Ÿè®¡
- éªŒè¯é›†: å¾…ç»Ÿè®¡
- æ€»è®¡: å¾…ç»Ÿè®¡

## ä¸‹è½½æ—¶é—´
{Path(__file__).stat().st_mtime}

## é¢„å¤„ç†è¯´æ˜
{config.get('preprocessing', 'æ— ç‰¹æ®Šé¢„å¤„ç†éœ€æ±‚')}
"""
        
        readme_path = dataset_path / "README.md"
        readme_path.write_text(readme_content, encoding='utf-8')

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®é›†ä¸‹è½½å’Œç®¡ç†å·¥å…·")
    parser.add_argument("--dataset", type=str, help="è¦ä¸‹è½½çš„æ•°æ®é›†åç§°")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½")
    parser.add_argument("--data-root", type=str, default="data", help="æ•°æ®æ ¹ç›®å½•")
    
    args = parser.parse_args()
    
    downloader = DatasetDownloader(args.data_root)
    
    if args.list:
        downloader.list_datasets()
    elif args.dataset:
        success = downloader.download_dataset(args.dataset, args.force)
        if success:
            print(f"\nğŸ‰ æ•°æ®é›† {args.dataset} å‡†å¤‡å°±ç»ª!")
            print(f"   è·¯å¾„: {Path(args.data_root) / args.dataset}")
        else:
            print(f"\nâŒ æ•°æ®é›† {args.dataset} ä¸‹è½½å¤±è´¥")
            sys.exit(1)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()