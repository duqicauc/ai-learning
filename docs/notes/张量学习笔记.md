这段代码虽然短，却把“深度学习一次迭代到底干了啥”全部拆给你看了。老师想让你牢牢记住的，就是下面这 5 个关键知识点——每一条都对应代码里的一行或几行。

---

### 1. 张量（Tensor）是“带梯度的数据容器”
```python
w = torch.tensor(2.0, requires_grad=True)
b = torch.tensor(1.0, requires_grad=True)
```
- 标量也是 0-D 张量；`requires_grad=True` 告诉 PyTorch：**“后面凡是用到它的运算，都给我建图，我要梯度！”**  
- 这是后续自动求导能发生的**前提**。

---

### 2. 前向传播（Forward）= 写公式即可
```python
y = w * x + b          # 线性模型
loss = (y - y_true)**2 # 均方误差
```
- 你只写了“预测值怎么来、损失怎么算”，**不需要手写 ∂loss/∂w**。  
- 这一串运算被 PyTorch 实时记录成**动态计算图**。

---

### 3. 自动求导（Autograd）（核心）
```python
loss.backward()
```
- 一行代码完成**链式法则**的反向传递：  
  ∂loss/∂y → ∂y/∂w → 累得到 ∂loss/∂w  
- 结果直接写进叶子张量的 `.grad` 属性里，**数值精确**、**速度≈手写**。

---

### 4. 梯度存在哪？—— `.grad` 属性
```python
print(w.grad)  # tensor(6.)
print(b.grad)  # tensor(2.)
```
- 让你**肉眼可见**地看到“每个参数该往哪个方向、迈多大步”才能减小 loss。  
- 后续优化器（SGD、Adam）就是读这个 `.grad` 来做 `param -= lr * grad`。

---

### 5. 深度学习“最小训练闭环”长这样
| 步骤 | 代码片段 |
|------|----------|
| ① 初始化可训练参数 | `w = tensor(..., requires_grad=True)` |
| ② 前向计算预测值 | `y = w*x + b` |
| ③ 计算损失 | `loss = (y - y_true)**2` |
| ④ 反向求梯度 | `loss.backward()` |
| ⑤ 参数更新（可接 `optimizer.step()`） | 本例省略，但逻辑位置在此 |

把这 5 步背下来，你就拥有了**用 PyTorch 手写任何复杂网络的基本骨架**。老师把“大作业”拆成了最小可运行单元，就是想让你先把这个骨架刻进肌肉记忆。