"""
ç®€åŒ–ç¥ç»ç½‘ç»œå›å½’å®ç°
ä½¿ç”¨çº¯NumPyå®ç°å¤šå±‚æ„ŸçŸ¥æœºè¿›è¡Œå›å½’é¢„æµ‹ï¼Œé¿å…PyTorchç¯å¢ƒä¾èµ–

åŠŸèƒ½:
- çº¯NumPyå®ç°çš„ç¥ç»ç½‘ç»œ
- ä¸linear_regression.pyä½¿ç”¨ç›¸åŒæ•°æ®
- å¯è§†åŒ–å¯¹æ¯”ç»“æœ
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

class SimpleNeuralNetwork:
    """ç®€å•çš„ç¥ç»ç½‘ç»œå®ç°"""
    
    def __init__(self, input_size=1, hidden_size=32, output_size=1):
        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        self.W1 = np.random.randn(input_size, hidden_size) * 0.1
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.1
        self.b2 = np.zeros((1, output_size))
        
        # å­˜å‚¨è®­ç»ƒå†å²
        self.losses = []
    
    def relu(self, x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """ReLUå¯¼æ•°"""
        return (x > 0).astype(float)
    
    def forward(self, X):
        """å‰å‘ä¼ æ’­"""
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.relu(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        return self.z2
    
    def backward(self, X, y, output, learning_rate=0.01):
        """åå‘ä¼ æ’­"""
        m = X.shape[0]
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        dz2 = output - y.reshape(-1, 1)
        dW2 = (1/m) * np.dot(self.a1.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)
        
        # éšè—å±‚æ¢¯åº¦
        da1 = np.dot(dz2, self.W2.T)
        dz1 = da1 * self.relu_derivative(self.z1)
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)
        
        # æ›´æ–°å‚æ•°
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
    
    def train(self, X, y, epochs=1000, learning_rate=0.01):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            output = self.forward(X)
            
            # è®¡ç®—æŸå¤±
            loss = np.mean((output.flatten() - y) ** 2)
            self.losses.append(loss)
            
            # åå‘ä¼ æ’­
            self.backward(X, y, output, learning_rate)
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 200 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}")
    
    def predict(self, X):
        """é¢„æµ‹"""
        return self.forward(X).flatten()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç¥ç»ç½‘ç»œå›å½’å¯¹æ¯”å®éªŒ\n")
    
    # ============================
    # æ•°æ®å‡†å¤‡
    # ============================
    
    print("ğŸ“Š å‡†å¤‡æ•°æ®...")
    
    # ä½¿ç”¨ä¸linear_regression.pyç›¸åŒçš„æ•°æ®
    x_data = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]).reshape(-1, 1)
    y_data = np.array([3.1, 5.0, 7.2, 9.1, 11.0, 13.1, 15.0, 16.8, 19.2, 21.0])
    
    print(f"æ•°æ®å½¢çŠ¶: X={x_data.shape}, y={y_data.shape}")
    print(f"æ•°æ®èŒƒå›´: Xâˆˆ[{x_data.min():.1f}, {x_data.max():.1f}], yâˆˆ[{y_data.min():.1f}, {y_data.max():.1f}]")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    x_scaled = scaler_x.fit_transform(x_data)
    y_scaled = scaler_y.fit_transform(y_data.reshape(-1, 1)).flatten()
    
    # ============================
    # çº¿æ€§å›å½’åŸºå‡†
    # ============================
    
    print("\nğŸ”µ è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹...")
    
    lr_model = LinearRegression()
    lr_model.fit(x_data, y_data)
    y_pred_lr = lr_model.predict(x_data)
    
    # çº¿æ€§å›å½’æ€§èƒ½
    mse_lr = mean_squared_error(y_data, y_pred_lr)
    r2_lr = r2_score(y_data, y_pred_lr)
    
    print(f"çº¿æ€§å›å½’ç»“æœ: y = {lr_model.coef_[0]:.3f}x + {lr_model.intercept_:.3f}")
    print(f"çº¿æ€§å›å½’æ€§èƒ½: MSE={mse_lr:.4f}, RÂ²={r2_lr:.4f}")
    
    # ============================
    # ç¥ç»ç½‘ç»œè®­ç»ƒ
    # ============================
    
    print("\nğŸ”´ è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹...")
    
    # åˆ›å»ºç¥ç»ç½‘ç»œ
    nn_model = SimpleNeuralNetwork(input_size=1, hidden_size=32, output_size=1)
    
    # è®­ç»ƒ
    nn_model.train(x_scaled, y_scaled, epochs=1000, learning_rate=0.1)
    
    # é¢„æµ‹
    y_pred_scaled = nn_model.predict(x_scaled)
    y_pred_nn = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    # ç¥ç»ç½‘ç»œæ€§èƒ½
    mse_nn = mean_squared_error(y_data, y_pred_nn)
    r2_nn = r2_score(y_data, y_pred_nn)
    
    print(f"ç¥ç»ç½‘ç»œæ€§èƒ½: MSE={mse_nn:.4f}, RÂ²={r2_nn:.4f}")
    
    # ============================
    # æ€§èƒ½å¯¹æ¯”
    # ============================
    
    print("\n" + "="*50)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*50)
    
    print(f"{'æŒ‡æ ‡':<10} {'çº¿æ€§å›å½’':<12} {'ç¥ç»ç½‘ç»œ':<12} {'æ”¹è¿›':<10}")
    print("-" * 50)
    
    mse_improvement = (mse_lr - mse_nn) / mse_lr * 100
    r2_improvement = (r2_nn - r2_lr) / r2_lr * 100
    
    print(f"{'MSE':<10} {mse_lr:<12.4f} {mse_nn:<12.4f} {mse_improvement:>+7.1f}%")
    print(f"{'RÂ²':<10} {r2_lr:<12.4f} {r2_nn:<12.4f} {r2_improvement:>+7.1f}%")
    
    # ============================
    # å¯è§†åŒ–ç»“æœ
    # ============================
    
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    
    # åˆ›å»ºå¯†é›†çš„æµ‹è¯•ç‚¹
    x_plot = np.linspace(0.5, 10.5, 100).reshape(-1, 1)
    
    # çº¿æ€§å›å½’é¢„æµ‹
    y_plot_lr = lr_model.predict(x_plot)
    
    # ç¥ç»ç½‘ç»œé¢„æµ‹
    x_plot_scaled = scaler_x.transform(x_plot)
    y_plot_scaled = nn_model.predict(x_plot_scaled)
    y_plot_nn = scaler_y.inverse_transform(y_plot_scaled.reshape(-1, 1)).flatten()
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ç¥ç»ç½‘ç»œå›å½’ vs çº¿æ€§å›å½’å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    # 1. é¢„æµ‹ç»“æœå¯¹æ¯”
    ax1 = axes[0, 0]
    ax1.scatter(x_data.flatten(), y_data, color='black', s=80, alpha=0.8, label='çœŸå®æ•°æ®', zorder=5)
    ax1.plot(x_plot.flatten(), y_plot_lr, color='blue', linewidth=2, label='çº¿æ€§å›å½’', alpha=0.8)
    ax1.plot(x_plot.flatten(), y_plot_nn, color='red', linewidth=2, label='ç¥ç»ç½‘ç»œ', alpha=0.8)
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_title('é¢„æµ‹ç»“æœå¯¹æ¯”')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. è®­ç»ƒæŸå¤±æ›²çº¿
    ax2 = axes[0, 1]
    ax2.plot(nn_model.losses, color='red', linewidth=1.5)
    ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax2.set_ylabel('æŸå¤±å€¼ (MSE)')
    ax2.set_title('ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹')
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3)
    
    # 3. æ®‹å·®åˆ†æ
    ax3 = axes[1, 0]
    residuals_lr = y_data - y_pred_lr
    residuals_nn = y_data - y_pred_nn
    ax3.scatter(y_pred_lr, residuals_lr, color='blue', alpha=0.7, label='çº¿æ€§å›å½’', s=60)
    ax3.scatter(y_pred_nn, residuals_nn, color='red', alpha=0.7, label='ç¥ç»ç½‘ç»œ', s=60)
    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax3.set_xlabel('é¢„æµ‹å€¼')
    ax3.set_ylabel('æ®‹å·®')
    ax3.set_title('æ®‹å·®åˆ†æ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    ax4 = axes[1, 1]
    metrics = ['MSE', 'RÂ²']
    lr_values = [mse_lr, r2_lr]
    nn_values = [mse_nn, r2_nn]
    
    x_pos = np.arange(len(metrics))
    width = 0.35
    
    bars1 = ax4.bar(x_pos - width/2, lr_values, width, label='çº¿æ€§å›å½’', color='blue', alpha=0.7)
    bars2 = ax4.bar(x_pos + width/2, nn_values, width, label='ç¥ç»ç½‘ç»œ', color='red', alpha=0.7)
    
    ax4.set_xlabel('è¯„ä¼°æŒ‡æ ‡')
    ax4.set_ylabel('æŒ‡æ ‡å€¼')
    ax4.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels(metrics)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    for bar in bars2:
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                 f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.show()
    
    # ============================
    # æ€»ç»“
    # ============================
    
    print("\n" + "="*60)
    print("ğŸ“‹ å®éªŒæ€»ç»“")
    print("="*60)
    
    print(f"""
ğŸ¯ å®éªŒç›®æ ‡: ä½¿ç”¨ç¥ç»ç½‘ç»œè§£å†³çº¿æ€§å›å½’é—®é¢˜

ğŸ“Š æ•°æ®é›†ä¿¡æ¯:
  - æ ·æœ¬æ•°é‡: {len(x_data)}
  - ç‰¹å¾ç»´åº¦: 1
  - æ•°æ®èŒƒå›´: xâˆˆ[{x_data.min():.1f}, {x_data.max():.1f}], yâˆˆ[{y_data.min():.1f}, {y_data.max():.1f}]

ğŸ—ï¸ ç¥ç»ç½‘ç»œæ¶æ„:
  - è¾“å…¥å±‚: 1ä¸ªç¥ç»å…ƒ
  - éšè—å±‚: 32ä¸ªç¥ç»å…ƒ (ReLUæ¿€æ´»)
  - è¾“å‡ºå±‚: 1ä¸ªç¥ç»å…ƒ
  - å®ç°æ–¹å¼: çº¯NumPy

ğŸ“ˆ æ€§èƒ½å¯¹æ¯”:
  çº¿æ€§å›å½’ - MSE: {mse_lr:.4f}, RÂ²: {r2_lr:.4f}
  ç¥ç»ç½‘ç»œ - MSE: {mse_nn:.4f}, RÂ²: {r2_nn:.4f}
  
ğŸ’¡ å…³é”®å‘ç°:
  1. å¯¹äºç®€å•çº¿æ€§å…³ç³»ï¼Œä¸¤ç§æ–¹æ³•æ€§èƒ½ç›¸è¿‘
  2. ç¥ç»ç½‘ç»œå…·æœ‰å­¦ä¹ éçº¿æ€§å…³ç³»çš„æ½œåŠ›
  3. æ•°æ®é‡è¾ƒå°æ—¶ï¼Œçº¿æ€§å›å½’å¯èƒ½æ›´ç¨³å®š
  4. ç¥ç»ç½‘ç»œéœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´

ğŸš€ æ”¹è¿›å»ºè®®:
  1. å°è¯•æ›´å¤æ‚çš„éçº¿æ€§æ•°æ®
  2. å¢åŠ æ•°æ®é‡ä»¥å‘æŒ¥ç¥ç»ç½‘ç»œä¼˜åŠ¿
  3. è°ƒæ•´ç½‘ç»œæ¶æ„å’Œè¶…å‚æ•°
  4. ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆ
""")
    
    print("="*60)
    print("âœ… å®éªŒå®Œæˆï¼")

if __name__ == "__main__":
    main()