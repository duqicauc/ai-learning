"""
ç¥ç»ç½‘ç»œå›å½’é¢„æµ‹å®ç°
ä½¿ç”¨PyTorchæ„å»ºå¤šå±‚æ„ŸçŸ¥æœº(MLP)æ¥è§£å†³ä¸linear_regression.pyç›¸åŒçš„å›å½’é—®é¢˜

å…³é”®ç»“è®ºï¼šå¯¹äºç®€å•çš„çº¿æ€§æ•°æ®è§„å¾‹ï¼Œé€‚åˆç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œç¥ç»ç½‘ç»œé€‚åˆå¤„ç†å¤æ‚çš„æ•°æ®åˆ†ç±»æˆ–é¢„æµ‹ä»»åŠ¡ã€‚

ä½œè€…: AI Learning Project and é˜¿æœ
åŠŸèƒ½: 
- ä½¿ç”¨ç›¸åŒçš„æ•°æ®é›†è¿›è¡Œç¥ç»ç½‘ç»œå›å½’
- å¯¹æ¯”çº¿æ€§å›å½’å’Œç¥ç»ç½‘ç»œçš„æ•ˆæœ
- å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹å’Œé¢„æµ‹ç»“æœ
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
torch.manual_seed(42)
np.random.seed(42)

# ============================
# ç¬¬ä¸€æ­¥ï¼šæ•°æ®å‡†å¤‡
# ============================

print("ğŸ” å‡†å¤‡æ•°æ®...")

# ä½¿ç”¨ä¸linear_regression.pyç›¸åŒçš„æ•°æ®
x_data = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]).reshape(-1, 1)
y_data = np.array([3.1, 5.0, 7.2, 9.1, 11.0, 13.1, 15.0, 16.8, 19.2, 21.0])

print(f"æ•°æ®å½¢çŠ¶: X={x_data.shape}, y={y_data.shape}")
print(f"XèŒƒå›´: [{x_data.min():.1f}, {x_data.max():.1f}]")
print(f"yèŒƒå›´: [{y_data.min():.1f}, {y_data.max():.1f}]")

# æ•°æ®æ ‡å‡†åŒ–ï¼ˆç¥ç»ç½‘ç»œè®­ç»ƒçš„æœ€ä½³å®è·µï¼‰
scaler_x = StandardScaler()
scaler_y = StandardScaler()

x_scaled = scaler_x.fit_transform(x_data)
y_scaled = scaler_y.fit_transform(y_data.reshape(-1, 1)).flatten()

print(f"æ ‡å‡†åŒ–å - XèŒƒå›´: [{x_scaled.min():.2f}, {x_scaled.max():.2f}]")
print(f"æ ‡å‡†åŒ–å - yèŒƒå›´: [{y_scaled.min():.2f}, {y_scaled.max():.2f}]")

# è½¬æ¢ä¸ºPyTorchå¼ é‡
X_tensor = torch.FloatTensor(x_scaled)
y_tensor = torch.FloatTensor(y_scaled)

# ============================
# ç¬¬äºŒæ­¥ï¼šå®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
# ============================

class RegressionMLP(nn.Module):
    """
    å¤šå±‚æ„ŸçŸ¥æœºå›å½’æ¨¡å‹
    
    æ¶æ„:
    - è¾“å…¥å±‚: 1ä¸ªç‰¹å¾
    - éšè—å±‚1: 64ä¸ªç¥ç»å…ƒ + ReLUæ¿€æ´»
    - éšè—å±‚2: 32ä¸ªç¥ç»å…ƒ + ReLUæ¿€æ´»  
    - éšè—å±‚3: 16ä¸ªç¥ç»å…ƒ + ReLUæ¿€æ´»
    - è¾“å‡ºå±‚: 1ä¸ªç¥ç»å…ƒï¼ˆå›å½’è¾“å‡ºï¼‰
    """
    
    def __init__(self, input_size=1, hidden_sizes=[64, 32, 16], output_size=1):
        super(RegressionMLP, self).__init__()
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_size = input_size
        
        # æ·»åŠ éšè—å±‚
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))  # æ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
            prev_size = hidden_size
        
        # æ·»åŠ è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))
        
        # ç»„åˆæ‰€æœ‰å±‚
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ä½¿ç”¨Xavieråˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        return self.network(x)

# ============================
# ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒé…ç½®
# ============================

print("\nğŸ—ï¸ åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")

# åˆ›å»ºæ¨¡å‹
model = RegressionMLP(input_size=1, hidden_sizes=[64, 32, 16], output_size=1)

# æ‰“å°æ¨¡å‹ç»“æ„
print("æ¨¡å‹æ¶æ„:")
print(model)

# è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
print(f"æ€»å‚æ•°æ•°: {total_params:,}")
print(f"å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}")

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±
optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=100
    )

# ============================
# ç¬¬å››æ­¥ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ
# ============================

print("\nğŸš€ å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œ...")

# è®­ç»ƒé…ç½®
num_epochs = 1000
train_losses = []
learning_rates = []

# è®­ç»ƒå¾ªç¯
model.train()
for epoch in range(num_epochs):
    # å‰å‘ä¼ æ’­
    outputs = model(X_tensor)
    loss = criterion(outputs.squeeze(), y_tensor)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    
    # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # æ›´æ–°å‚æ•°
    optimizer.step()
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(loss)
    
    # è®°å½•è®­ç»ƒä¿¡æ¯
    train_losses.append(loss.item())
    learning_rates.append(optimizer.param_groups[0]['lr'])
    
    # æ‰“å°è®­ç»ƒè¿›åº¦
    if (epoch + 1) % 100 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], "
              f"Loss: {loss.item():.6f}, "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")

print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŸå¤±: {train_losses[-1]:.6f}")

# ============================
# ç¬¬äº”æ­¥ï¼šæ¨¡å‹è¯„ä¼°å’Œé¢„æµ‹
# ============================

print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

# åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
model.eval()

# åœ¨åŸå§‹æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹
with torch.no_grad():
    # æ ‡å‡†åŒ–è¾“å…¥
    x_test_scaled = scaler_x.transform(x_data)
    X_test_tensor = torch.FloatTensor(x_test_scaled)
    
    # ç¥ç»ç½‘ç»œé¢„æµ‹ï¼ˆæ ‡å‡†åŒ–è¾“å‡ºï¼‰
    y_pred_scaled = model(X_test_tensor).squeeze().numpy()
    
    # åæ ‡å‡†åŒ–å¾—åˆ°åŸå§‹å°ºåº¦çš„é¢„æµ‹
    y_pred_nn = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
mse = mean_squared_error(y_data, y_pred_nn)
rmse = np.sqrt(mse)
r2 = r2_score(y_data, y_pred_nn)

print(f"ç¥ç»ç½‘ç»œæ€§èƒ½æŒ‡æ ‡:")
print(f"  å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
print(f"  å†³å®šç³»æ•° (RÂ²): {r2:.4f}")

# ============================
# ç¬¬å…­æ­¥ï¼šä¸çº¿æ€§å›å½’å¯¹æ¯”
# ============================

print("\nğŸ” ä¸çº¿æ€§å›å½’å¯¹æ¯”...")

# ä½¿ç”¨sklearnçš„çº¿æ€§å›å½’ä½œä¸ºåŸºå‡†
from sklearn.linear_model import LinearRegression

lr_model = LinearRegression()
lr_model.fit(x_data, y_data)
y_pred_lr = lr_model.predict(x_data)

# çº¿æ€§å›å½’æ€§èƒ½
mse_lr = mean_squared_error(y_data, y_pred_lr)
rmse_lr = np.sqrt(mse_lr)
r2_lr = r2_score(y_data, y_pred_lr)

print(f"çº¿æ€§å›å½’æ€§èƒ½æŒ‡æ ‡:")
print(f"  å‡æ–¹è¯¯å·® (MSE): {mse_lr:.4f}")
print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_lr:.4f}")
print(f"  å†³å®šç³»æ•° (RÂ²): {r2_lr:.4f}")

print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
print(f"  MSEæ”¹è¿›: {((mse_lr - mse) / mse_lr * 100):+.2f}%")
print(f"  RMSEæ”¹è¿›: {((rmse_lr - rmse) / rmse_lr * 100):+.2f}%")
print(f"  RÂ²æ”¹è¿›: {((r2 - r2_lr) / r2_lr * 100):+.2f}%")

# ============================
# ç¬¬ä¸ƒæ­¥ï¼šç»“æœå¯è§†åŒ–
# ============================

print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")

# åˆ›å»ºæ›´å¯†é›†çš„æµ‹è¯•ç‚¹ç”¨äºç»˜åˆ¶å¹³æ»‘æ›²çº¿
x_plot = np.linspace(0.5, 10.5, 100).reshape(-1, 1)
x_plot_scaled = scaler_x.transform(x_plot)
X_plot_tensor = torch.FloatTensor(x_plot_scaled)

with torch.no_grad():
    y_plot_scaled = model(X_plot_tensor).squeeze().numpy()
    y_plot_nn = scaler_y.inverse_transform(y_plot_scaled.reshape(-1, 1)).flatten()

y_plot_lr = lr_model.predict(x_plot)

# åˆ›å»ºç»¼åˆå¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('ç¥ç»ç½‘ç»œå›å½’ vs çº¿æ€§å›å½’å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')

# 1. é¢„æµ‹ç»“æœå¯¹æ¯”
ax1 = axes[0, 0]
ax1.scatter(x_data.flatten(), y_data, color='black', s=80, alpha=0.8, label='çœŸå®æ•°æ®', zorder=5)
ax1.plot(x_plot.flatten(), y_plot_lr, color='red', linewidth=2, label='çº¿æ€§å›å½’', alpha=0.8)
ax1.plot(x_plot.flatten(), y_plot_nn, color='blue', linewidth=2, label='ç¥ç»ç½‘ç»œ', alpha=0.8)
ax1.set_xlabel('x (ç‰¹å¾)')
ax1.set_ylabel('y (ç›®æ ‡å€¼)')
ax1.set_title('é¢„æµ‹ç»“æœå¯¹æ¯”')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. è®­ç»ƒæŸå¤±æ›²çº¿
ax2 = axes[0, 1]
ax2.plot(train_losses, color='blue', linewidth=1.5)
ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
ax2.set_ylabel('æŸå¤±å€¼ (MSE)')
ax2.set_title('ç¥ç»ç½‘ç»œè®­ç»ƒæŸå¤±æ›²çº¿')
ax2.set_yscale('log')
ax2.grid(True, alpha=0.3)

# 3. æ®‹å·®åˆ†æ
ax3 = axes[1, 0]
residuals_lr = y_data - y_pred_lr
residuals_nn = y_data - y_pred_nn
ax3.scatter(y_pred_lr, residuals_lr, color='red', alpha=0.7, label='çº¿æ€§å›å½’')
ax3.scatter(y_pred_nn, residuals_nn, color='blue', alpha=0.7, label='ç¥ç»ç½‘ç»œ')
ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)
ax3.set_xlabel('é¢„æµ‹å€¼')
ax3.set_ylabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)')
ax3.set_title('æ®‹å·®åˆ†æ')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
ax4 = axes[1, 1]
metrics = ['MSE', 'RMSE', 'RÂ²']
lr_values = [mse_lr, rmse_lr, r2_lr]
nn_values = [mse, rmse, r2]

x_pos = np.arange(len(metrics))
width = 0.35

bars1 = ax4.bar(x_pos - width/2, lr_values, width, label='çº¿æ€§å›å½’', color='red', alpha=0.7)
bars2 = ax4.bar(x_pos + width/2, nn_values, width, label='ç¥ç»ç½‘ç»œ', color='blue', alpha=0.7)

ax4.set_xlabel('è¯„ä¼°æŒ‡æ ‡')
ax4.set_ylabel('æŒ‡æ ‡å€¼')
ax4.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
ax4.set_xticks(x_pos)
ax4.set_xticklabels(metrics)
ax4.legend()
ax4.grid(True, alpha=0.3)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar in bars1:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.3f}', ha='center', va='bottom', fontsize=9)

for bar in bars2:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.3f}', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

# ============================
# ç¬¬å…«æ­¥ï¼šæ€»ç»“å’Œå»ºè®®
# ============================

print("\n" + "="*60)
print("ğŸ“‹ å®éªŒæ€»ç»“")
print("="*60)

print(f"""
ğŸ¯ å®éªŒç›®æ ‡: ä½¿ç”¨ç¥ç»ç½‘ç»œè§£å†³çº¿æ€§å›å½’é—®é¢˜

ğŸ“Š æ•°æ®é›†ä¿¡æ¯:
  - æ ·æœ¬æ•°é‡: {len(x_data)}
  - ç‰¹å¾ç»´åº¦: 1
  - æ•°æ®èŒƒå›´: xâˆˆ[{x_data.min():.1f}, {x_data.max():.1f}], yâˆˆ[{y_data.min():.1f}, {y_data.max():.1f}]

ğŸ—ï¸ ç¥ç»ç½‘ç»œæ¶æ„:
  - è¾“å…¥å±‚: 1ä¸ªç¥ç»å…ƒ
  - éšè—å±‚: 64 â†’ 32 â†’ 16 ç¥ç»å…ƒ
  - è¾“å‡ºå±‚: 1ä¸ªç¥ç»å…ƒ
  - æ¿€æ´»å‡½æ•°: ReLU
  - æ­£åˆ™åŒ–: Dropout(0.1)
  - æ€»å‚æ•°: {total_params:,}

ğŸ“ˆ æ€§èƒ½å¯¹æ¯”:
  çº¿æ€§å›å½’ - MSE: {mse_lr:.4f}, RMSE: {rmse_lr:.4f}, RÂ²: {r2_lr:.4f}
  ç¥ç»ç½‘ç»œ - MSE: {mse:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}
  
ğŸ’¡ å…³é”®å‘ç°:
  1. å¯¹äºç®€å•çš„çº¿æ€§å…³ç³»ï¼Œç¥ç»ç½‘ç»œå¯ä»¥è¾¾åˆ°ä¸çº¿æ€§å›å½’ç›¸è¿‘çš„æ€§èƒ½
  2. ç¥ç»ç½‘ç»œå…·æœ‰æ›´å¼ºçš„éçº¿æ€§æ‹Ÿåˆèƒ½åŠ›ï¼Œé€‚åˆå¤æ‚æ•°æ®
  3. éœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æº
  4. é€šè¿‡é€‚å½“çš„æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ

ğŸš€ æ”¹è¿›å»ºè®®:
  1. å¢åŠ æ•°æ®é‡ä»¥å……åˆ†å‘æŒ¥ç¥ç»ç½‘ç»œä¼˜åŠ¿
  2. å°è¯•ä¸åŒçš„ç½‘ç»œæ¶æ„å’Œè¶…å‚æ•°
  3. ä½¿ç”¨æ›´å¤æ‚çš„éçº¿æ€§æ•°æ®è¿›è¡Œæµ‹è¯•
  4. è€ƒè™‘ä½¿ç”¨é›†æˆæ–¹æ³•ç»“åˆå¤šä¸ªæ¨¡å‹
""")

print("="*60)
print("âœ… å®éªŒå®Œæˆï¼")