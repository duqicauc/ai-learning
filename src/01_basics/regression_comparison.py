"""
å›å½’æ–¹æ³•å¯¹æ¯”åˆ†æ
å¿«é€Ÿæ¯”è¾ƒçº¿æ€§å›å½’å’Œç¥ç»ç½‘ç»œåœ¨ç›¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°

åŠŸèƒ½:
- å¹¶æ’è¿è¡Œä¸¤ç§æ–¹æ³•
- ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
- å¯è§†åŒ–ç»“æœå·®å¼‚
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

def prepare_data():
    """å‡†å¤‡æ•°æ®"""
    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]).reshape(-1, 1)
    y = np.array([3.1, 5.0, 7.2, 9.1, 11.0, 13.1, 15.0, 16.8, 19.2, 21.0])
    return x, y

def linear_regression_method(x, y):
    """çº¿æ€§å›å½’æ–¹æ³•"""
    print("ğŸ”µ è¿è¡Œçº¿æ€§å›å½’...")
    start_time = time.time()
    
    model = LinearRegression()
    model.fit(x, y)
    y_pred = model.predict(x)
    
    training_time = time.time() - start_time
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    
    return {
        'model': model,
        'predictions': y_pred,
        'mse': mse,
        'r2': r2,
        'training_time': training_time,
        'params': f"w={model.coef_[0]:.3f}, b={model.intercept_:.3f}"
    }

class SimpleNN(nn.Module):
    """ç®€åŒ–çš„ç¥ç»ç½‘ç»œ"""
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )
    
    def forward(self, x):
        return self.network(x)

def neural_network_method(x, y):
    """ç¥ç»ç½‘ç»œæ–¹æ³•"""
    print("ğŸ”´ è¿è¡Œç¥ç»ç½‘ç»œ...")
    start_time = time.time()
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    x_scaled = scaler_x.fit_transform(x)
    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_tensor = torch.FloatTensor(x_scaled)
    y_tensor = torch.FloatTensor(y_scaled)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleNN()
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # è®­ç»ƒ
    model.train()
    losses = []
    for epoch in range(500):
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = criterion(outputs.squeeze(), y_tensor)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        y_pred_scaled = model(X_tensor).squeeze().numpy()
        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    
    training_time = time.time() - start_time
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    
    return {
        'model': model,
        'predictions': y_pred,
        'mse': mse,
        'r2': r2,
        'training_time': training_time,
        'losses': losses,
        'params': f"{total_params} ä¸ªå‚æ•°",
        'scalers': (scaler_x, scaler_y)
    }

def compare_methods():
    """æ¯”è¾ƒä¸¤ç§æ–¹æ³•"""
    print("ğŸš€ å¼€å§‹å›å½’æ–¹æ³•å¯¹æ¯”åˆ†æ\n")
    
    # å‡†å¤‡æ•°æ®
    x, y = prepare_data()
    print(f"ğŸ“Š æ•°æ®é›†: {len(x)} ä¸ªæ ·æœ¬, ç‰¹å¾èŒƒå›´ [{x.min():.1f}, {x.max():.1f}]")
    print(f"ç›®æ ‡å€¼èŒƒå›´ [{y.min():.1f}, {y.max():.1f}]\n")
    
    # è¿è¡Œä¸¤ç§æ–¹æ³•
    lr_results = linear_regression_method(x, y)
    nn_results = neural_network_method(x, y)
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "="*50)
    print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*50)
    
    print(f"{'æŒ‡æ ‡':<15} {'çº¿æ€§å›å½’':<15} {'ç¥ç»ç½‘ç»œ':<15} {'æ”¹è¿›':<10}")
    print("-" * 60)
    
    # MSEå¯¹æ¯”
    mse_improvement = (lr_results['mse'] - nn_results['mse']) / lr_results['mse'] * 100
    print(f"{'MSE':<15} {lr_results['mse']:<15.4f} {nn_results['mse']:<15.4f} {mse_improvement:>+7.1f}%")
    
    # RÂ²å¯¹æ¯”
    r2_improvement = (nn_results['r2'] - lr_results['r2']) / lr_results['r2'] * 100
    print(f"{'RÂ²':<15} {lr_results['r2']:<15.4f} {nn_results['r2']:<15.4f} {r2_improvement:>+7.1f}%")
    
    # è®­ç»ƒæ—¶é—´å¯¹æ¯”
    time_ratio = nn_results['training_time'] / lr_results['training_time']
    print(f"{'è®­ç»ƒæ—¶é—´(s)':<15} {lr_results['training_time']:<15.4f} {nn_results['training_time']:<15.4f} {time_ratio:>7.1f}x")
    
    print(f"{'æ¨¡å‹å‚æ•°':<15} {lr_results['params']:<15} {nn_results['params']:<15}")
    
    # å¯è§†åŒ–å¯¹æ¯”
    visualize_comparison(x, y, lr_results, nn_results)
    
    return lr_results, nn_results

def visualize_comparison(x, y, lr_results, nn_results):
    """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
    
    # åˆ›å»ºå¯†é›†çš„æµ‹è¯•ç‚¹
    x_plot = np.linspace(0.5, 10.5, 100).reshape(-1, 1)
    
    # çº¿æ€§å›å½’é¢„æµ‹
    y_plot_lr = lr_results['model'].predict(x_plot)
    
    # ç¥ç»ç½‘ç»œé¢„æµ‹
    scaler_x, scaler_y = nn_results['scalers']
    x_plot_scaled = scaler_x.transform(x_plot)
    X_plot_tensor = torch.FloatTensor(x_plot_scaled)
    
    nn_results['model'].eval()
    with torch.no_grad():
        y_plot_scaled = nn_results['model'](X_plot_tensor).squeeze().numpy()
        y_plot_nn = scaler_y.inverse_transform(y_plot_scaled.reshape(-1, 1)).flatten()
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. é¢„æµ‹ç»“æœå¯¹æ¯”
    ax1 = axes[0]
    ax1.scatter(x.flatten(), y, color='black', s=100, alpha=0.8, label='çœŸå®æ•°æ®', zorder=5)
    ax1.plot(x_plot.flatten(), y_plot_lr, color='blue', linewidth=2, label='çº¿æ€§å›å½’', alpha=0.8)
    ax1.plot(x_plot.flatten(), y_plot_nn, color='red', linewidth=2, label='ç¥ç»ç½‘ç»œ', alpha=0.8)
    ax1.set_xlabel('x')
    ax1.set_ylabel('y')
    ax1.set_title('é¢„æµ‹ç»“æœå¯¹æ¯”')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. æ®‹å·®å¯¹æ¯”
    ax2 = axes[1]
    residuals_lr = y - lr_results['predictions']
    residuals_nn = y - nn_results['predictions']
    
    ax2.scatter(lr_results['predictions'], residuals_lr, color='blue', alpha=0.7, label='çº¿æ€§å›å½’', s=80)
    ax2.scatter(nn_results['predictions'], residuals_nn, color='red', alpha=0.7, label='ç¥ç»ç½‘ç»œ', s=80)
    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax2.set_xlabel('é¢„æµ‹å€¼')
    ax2.set_ylabel('æ®‹å·®')
    ax2.set_title('æ®‹å·®åˆ†æ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ç¥ç»ç½‘ç»œè®­ç»ƒæŸå¤±
    ax3 = axes[2]
    ax3.plot(nn_results['losses'], color='red', linewidth=1.5)
    ax3.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax3.set_ylabel('æŸå¤±å€¼')
    ax3.set_title('ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹')
    ax3.set_yscale('log')
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    try:
        lr_results, nn_results = compare_methods()
        
        print("\nğŸ’¡ æ€»ç»“:")
        if nn_results['mse'] < lr_results['mse']:
            print("âœ… ç¥ç»ç½‘ç»œåœ¨MSEæŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½")
        else:
            print("âœ… çº¿æ€§å›å½’åœ¨MSEæŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½")
            
        if nn_results['training_time'] > lr_results['training_time'] * 2:
            print("âš ï¸  ç¥ç»ç½‘ç»œéœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´")
        else:
            print("âœ… ä¸¤ç§æ–¹æ³•è®­ç»ƒæ—¶é—´ç›¸è¿‘")
            
        print("\nğŸ¯ å»ºè®®:")
        print("- å¯¹äºç®€å•çº¿æ€§å…³ç³»ï¼Œçº¿æ€§å›å½’æ›´é«˜æ•ˆ")
        print("- å¯¹äºå¤æ‚éçº¿æ€§å…³ç³»ï¼Œç¥ç»ç½‘ç»œæ›´æœ‰ä¼˜åŠ¿")
        print("- æ•°æ®é‡è¾ƒå°æ—¶ï¼Œçº¿æ€§å›å½’å¯èƒ½æ›´ç¨³å®š")
        print("- æ•°æ®é‡è¾ƒå¤§æ—¶ï¼Œç¥ç»ç½‘ç»œæ½œåŠ›æ›´å¤§")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()