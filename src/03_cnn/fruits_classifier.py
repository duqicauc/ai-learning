# -*- coding: utf-8 -*-
"""
ğŸğŸŒ ç¬¬ä¹èŠ‚å®æˆ˜ï¼šCNN æ°´æœåˆ†ç±»å™¨ - 100ç§æ°´æœè¯†åˆ«

=== å¤šåˆ†ç±»CNNé¡¹ç›®è¯¦è§£ ===

ğŸ§  é¡¹ç›®æ¦‚è¿°ï¼š
æœ¬é¡¹ç›®åŸºäºcats_dogs_classifierçš„æˆåŠŸç»éªŒï¼Œæ‰©å±•åˆ°100ç§æ°´æœçš„å¤šåˆ†ç±»ä»»åŠ¡ã€‚
ç›¸æ¯”äºŒåˆ†ç±»ï¼Œå¤šåˆ†ç±»CNNéœ€è¦è€ƒè™‘æ›´å¤šå› ç´ ï¼š
- ç±»åˆ«æ•°é‡å¤§å¹…å¢åŠ ï¼ˆ2 â†’ 100ï¼‰
- ç±»é—´ç›¸ä¼¼æ€§æ›´é«˜ï¼ˆä¸åŒæ°´æœå¯èƒ½å¾ˆç›¸ä¼¼ï¼‰
- æ•°æ®ä¸å¹³è¡¡é—®é¢˜æ›´çªå‡º
- æ¨¡å‹å¤æ‚åº¦éœ€è¦ç›¸åº”æå‡

ğŸ—ï¸ æŠ€æœ¯å‡çº§ç‚¹ï¼š
1ï¸âƒ£ æ¨¡å‹æ¶æ„ä¼˜åŒ–ï¼š
   - å¢åŠ ç½‘ç»œæ·±åº¦å’Œå®½åº¦
   - ä½¿ç”¨Batch NormalizationåŠ é€Ÿè®­ç»ƒ
   - æ·»åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
   - è°ƒæ•´æœ€åå…¨è¿æ¥å±‚è¾“å‡ºä¸º100ç±»

2ï¸âƒ£ æ•°æ®å¤„ç†å¢å¼ºï¼š
   - æ›´å¼ºçš„æ•°æ®å¢å¼ºç­–ç•¥
   - ç±»åˆ«æƒé‡å¹³è¡¡
   - æ›´ç²¾ç»†çš„é¢„å¤„ç†æµç¨‹

3ï¸âƒ£ è®­ç»ƒç­–ç•¥ä¼˜åŒ–ï¼š
   - å­¦ä¹ ç‡è°ƒåº¦
   - æ—©åœæœºåˆ¶
   - æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜
   - è¯¦ç»†çš„è®­ç»ƒç›‘æ§

ğŸ“Š æ•°æ®é›†ä¿¡æ¯ï¼š
- æ¥æºï¼šModelScope fruits100æ•°æ®é›†
- ç±»åˆ«ï¼š100ç§ä¸åŒæ°´æœ
- æ ¼å¼ï¼šJPGå›¾ç‰‡ï¼ŒæŒ‰æ–‡ä»¶å¤¹åˆ†ç±»
- ç»“æ„ï¼štrain/ å’Œ val/ ç›®å½•

ğŸ¯ å­¦ä¹ ç›®æ ‡ï¼š
- æŒæ¡å¤šåˆ†ç±»CNNçš„è®¾è®¡åŸç†
- ç†è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ
- å­¦ä¼šå¤æ‚æ¨¡å‹çš„è®­ç»ƒæŠ€å·§
- å®ç°ç«¯åˆ°ç«¯çš„å¤šåˆ†ç±»é¡¹ç›®
"""

# ----------------------------
# ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥å¿…è¦åº“
# ----------------------------

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, WeightedRandomSampler
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import json
from collections import Counter
import time
from pathlib import Path

# ----------------------------
# ç¬¬äºŒæ­¥ï¼šè®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
# ----------------------------

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

if __name__ == '__main__':
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"âœ… GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # ----------------------------
    # ç¬¬ä¸‰æ­¥ï¼šæ•°æ®è·¯å¾„é…ç½®
    # ----------------------------

    print("ğŸ”„ æ­£åœ¨åŠ è½½æ°´æœæ•°æ®é›†é…ç½®...")

    # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®é…ç½®ç®¡ç†
    try:
        # å°è¯•ç›¸å¯¹å¯¼å…¥ï¼ˆå½“ä½œä¸ºåŒ…ä½¿ç”¨æ—¶ï¼‰
        from ..utils.data_config import get_fruits100_paths, data_config
    except ImportError:
        # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨ç»å¯¹å¯¼å…¥
        import sys
        sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
        from src.utils.data_config import get_fruits100_paths, data_config
    
    try:
        train_dir, val_dir, test_dir = get_fruits100_paths()
        
        print(f"ğŸ“ æ•°æ®è·¯å¾„:")
        print(f"  è®­ç»ƒé›†: {train_dir}")
        print(f"  éªŒè¯é›†: {val_dir}")
        print(f"  æµ‹è¯•é›†: {test_dir}")
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        dataset_info = data_config.get_dataset_info("fruits100")
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {dataset_info['description']}")
        print(f"ğŸ”— æ•°æ®æ¥æº: {dataset_info['source']}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†é…ç½®é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²ä¸‹è½½fruits100æ•°æ®é›†åˆ°æ­£ç¡®ä½ç½®")
        exit(1)

    # ========================================================================
    # ç¬¬å››æ­¥ï¼šæ•°æ®é¢„å¤„ç†å’Œå¢å¼º - å¤šåˆ†ç±»çš„å…³é”®ï¼
    # ========================================================================

    """
    ğŸŒŸ å¤šåˆ†ç±»æ•°æ®å¢å¼ºç­–ç•¥è¯¦è§£ï¼š
    
    ç›¸æ¯”äºŒåˆ†ç±»ï¼Œå¤šåˆ†ç±»éœ€è¦æ›´å¼ºçš„æ•°æ®å¢å¼ºæ¥ï¼š
    1. å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´é²æ£’çš„ç‰¹å¾
    2. å‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼ˆ100ä¸ªç±»åˆ«ï¼Œæ¨¡å‹å®¹æ˜“è®°ä½è®­ç»ƒæ•°æ®ï¼‰
    3. æé«˜æ¨¡å‹å¯¹å˜æ¢çš„ä¸å˜æ€§
    
    å¢å¼ºæŠ€æœ¯é€‰æ‹©ï¼š
    - RandomResizedCrop: éšæœºè£å‰ªå’Œç¼©æ”¾ï¼Œæ¨¡æ‹Ÿä¸åŒæ‹æ‘„è·ç¦»
    - RandomHorizontalFlip: æ°´å¹³ç¿»è½¬ï¼Œå¢åŠ è§†è§’å¤šæ ·æ€§
    - ColorJitter: é¢œè‰²æŠ–åŠ¨ï¼Œé€‚åº”ä¸åŒå…‰ç…§æ¡ä»¶
    - RandomRotation: å°è§’åº¦æ—‹è½¬ï¼Œé€‚åº”ä¸åŒæ‹æ‘„è§’åº¦
    - RandomAffine: ä»¿å°„å˜æ¢ï¼Œæ¨¡æ‹Ÿè½»å¾®çš„é€è§†å˜åŒ–
    """

    # è®­ç»ƒé›†æ•°æ®å¢å¼ºï¼ˆå¼ºå¢å¼ºï¼‰
    train_transform = transforms.Compose([
        # 1ï¸âƒ£ å°ºå¯¸è°ƒæ•´å’Œéšæœºè£å‰ª
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.8, 1.2)),
        
        # 2ï¸âƒ£ å‡ ä½•å˜æ¢
        transforms.RandomHorizontalFlip(p=0.5),  # 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬
        transforms.RandomRotation(degrees=15),    # Â±15åº¦éšæœºæ—‹è½¬
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        
        # 3ï¸âƒ£ é¢œè‰²å¢å¼º
        transforms.ColorJitter(
            brightness=0.2,    # äº®åº¦å˜åŒ–Â±20%
            contrast=0.2,      # å¯¹æ¯”åº¦å˜åŒ–Â±20%
            saturation=0.2,    # é¥±å’Œåº¦å˜åŒ–Â±20%
            hue=0.1           # è‰²è°ƒå˜åŒ–Â±10%
        ),
        
        # 4ï¸âƒ£ æ ‡å‡†åŒ–å¤„ç†
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # éªŒè¯/æµ‹è¯•é›†æ•°æ®é¢„å¤„ç†ï¼ˆæ— å¢å¼ºï¼‰
    val_test_transform = transforms.Compose([
        transforms.Resize(256),                    # å…ˆæ”¾å¤§
        transforms.CenterCrop(224),                # ä¸­å¿ƒè£å‰ªåˆ°æ ‡å‡†å°ºå¯¸
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # ----------------------------
    # ç¬¬äº”æ­¥ï¼šåˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    # ----------------------------

    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†...")

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = datasets.ImageFolder(root=str(train_dir), transform=train_transform)
    val_dataset = datasets.ImageFolder(root=str(val_dir), transform=val_test_transform)
    test_dataset = datasets.ImageFolder(root=str(test_dir), transform=val_test_transform)

    # è·å–ç±»åˆ«ä¿¡æ¯
    num_classes = len(train_dataset.classes)
    class_names = train_dataset.classes
    class_to_idx = train_dataset.class_to_idx

    print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"  éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} å¼ å›¾ç‰‡")
    print(f"  ç±»åˆ«æ•°é‡: {num_classes}")
    print(f"  å‰10ä¸ªç±»åˆ«: {class_names[:10]}")

    # åˆ†æç±»åˆ«åˆ†å¸ƒ
    print("\nğŸ“Š åˆ†æè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ...")
    train_targets = [train_dataset.targets[i] for i in range(len(train_dataset))]
    class_counts = Counter(train_targets)
    
    print(f"  æœ€å¤šæ ·æœ¬ç±»åˆ«: {max(class_counts.values())} å¼ ")
    print(f"  æœ€å°‘æ ·æœ¬ç±»åˆ«: {min(class_counts.values())} å¼ ")
    print(f"  å¹³å‡æ ·æœ¬æ•°é‡: {np.mean(list(class_counts.values())):.1f} å¼ ")

    # ========================================================================
    # ç¬¬å…­æ­¥ï¼šå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - å¤šåˆ†ç±»çš„é‡è¦æŠ€å·§ï¼
    # ========================================================================

    """
    ğŸ¯ ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜è§£å†³æ–¹æ¡ˆï¼š
    
    åœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸åŒç±»åˆ«çš„æ ·æœ¬æ•°é‡å¯èƒ½å·®å¼‚å¾ˆå¤§ï¼Œå¯¼è‡´ï¼š
    - æ¨¡å‹åå‘äºæ ·æœ¬å¤šçš„ç±»åˆ«
    - æ ·æœ¬å°‘çš„ç±»åˆ«è¯†åˆ«ç‡ä½
    - æ•´ä½“æ€§èƒ½ä¸‹é™
    
    è§£å†³æ–¹æ¡ˆï¼š
    1. WeightedRandomSampler: æŒ‰ç±»åˆ«æƒé‡é‡‡æ ·
    2. ç±»åˆ«æƒé‡æŸå¤±å‡½æ•°: ç»™å°‘æ•°ç±»åˆ«æ›´é«˜æƒé‡
    3. æ•°æ®å¢å¼º: ä¸ºå°‘æ•°ç±»åˆ«ç”Ÿæˆæ›´å¤šæ ·æœ¬
    """

    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆç”¨äºæŸå¤±å‡½æ•°ï¼‰
    class_weights = []
    total_samples = len(train_dataset)
    
    for i in range(num_classes):
        class_count = class_counts.get(i, 1)  # é¿å…é™¤é›¶
        weight = total_samples / (num_classes * class_count)
        class_weights.append(weight)
    
    class_weights = torch.FloatTensor(class_weights).to(device)
    print(f"âœ… ç±»åˆ«æƒé‡è®¡ç®—å®Œæˆï¼Œæƒé‡èŒƒå›´: {class_weights.min():.3f} - {class_weights.max():.3f}")

    # åˆ›å»ºåŠ æƒé‡‡æ ·å™¨ï¼ˆç”¨äºæ•°æ®åŠ è½½ï¼‰
    sample_weights = [class_weights[target] for target in train_targets]
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(train_dataset),
        replacement=True
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = 32  # å¤šåˆ†ç±»ä»»åŠ¡ï¼Œé€‚ä¸­çš„batch size
    num_workers = 0 if os.name == 'nt' else 4

    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        sampler=sampler,  # ä½¿ç”¨åŠ æƒé‡‡æ ·
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )

    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )

    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )

    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")

    # ========================================================================
    # ç¬¬ä¸ƒæ­¥ï¼šå®šä¹‰CNNæ¨¡å‹æ¶æ„ - å¤šåˆ†ç±»ä¸“ç”¨è®¾è®¡ï¼
    # ========================================================================

    class FruitsCNN(nn.Module):
        """
        ğŸ—ï¸ æ°´æœåˆ†ç±»CNNæ¶æ„è¯¦è§£
        
        ç›¸æ¯”äºŒåˆ†ç±»ï¼Œå¤šåˆ†ç±»CNNçš„è®¾è®¡è€ƒè™‘ï¼š
        1. æ›´æ·±çš„ç½‘ç»œï¼šæå–æ›´å¤æ‚çš„ç‰¹å¾
        2. æ›´å¤šçš„é€šé“ï¼šæ•è·æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤º
        3. Batch Normalizationï¼šåŠ é€Ÿè®­ç»ƒï¼Œæé«˜ç¨³å®šæ€§
        4. Dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
        5. æ®‹å·®è¿æ¥ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
        
        ç½‘ç»œç»“æ„ï¼š
        è¾“å…¥(224Ã—224Ã—3) 
        â†’ Conv Block 1 (64é€šé“) â†’ 112Ã—112Ã—64
        â†’ Conv Block 2 (128é€šé“) â†’ 56Ã—56Ã—128
        â†’ Conv Block 3 (256é€šé“) â†’ 28Ã—28Ã—256
        â†’ Conv Block 4 (512é€šé“) â†’ 14Ã—14Ã—512
        â†’ Global Average Pooling â†’ 512
        â†’ FC1(256) â†’ Dropout â†’ FC2(100) â†’ è¾“å‡º
        """
        
        def __init__(self, num_classes=100):
            super(FruitsCNN, self).__init__()
            
            # ===== å·ç§¯å—å®šä¹‰ =====
            
            # ğŸ” Conv Block 1: åŸºç¡€ç‰¹å¾æå–
            self.conv_block1 = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),  # BNåŠ é€Ÿè®­ç»ƒ
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2)  # 224â†’112
            )
            
            # ğŸ” Conv Block 2: ä¸­çº§ç‰¹å¾ç»„åˆ
            self.conv_block2 = nn.Sequential(
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.Conv2d(128, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2)  # 112â†’56
            )
            
            # ğŸ” Conv Block 3: é«˜çº§ç‰¹å¾æŠ½è±¡
            self.conv_block3 = nn.Sequential(
                nn.Conv2d(128, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2)  # 56â†’28
            )
            
            # ğŸ” Conv Block 4: æ·±å±‚ç‰¹å¾æå–
            self.conv_block4 = nn.Sequential(
                nn.Conv2d(256, 512, kernel_size=3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.Conv2d(512, 512, kernel_size=3, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2)  # 28â†’14
            )
            
            # ===== å…¨å±€å¹³å‡æ± åŒ– =====
            # ç›¸æ¯”Flattenï¼ŒGAPå‡å°‘å‚æ•°é‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
            
            # ===== åˆ†ç±»å™¨è®¾è®¡ =====
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),  # é˜²æ­¢è¿‡æ‹Ÿåˆ
                nn.Linear(512, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(0.3),
                nn.Linear(256, num_classes)  # è¾“å‡º100ä¸ªç±»åˆ«
            )
            
            # æƒé‡åˆå§‹åŒ–
            self._initialize_weights()
        
        def _initialize_weights(self):
            """æƒé‡åˆå§‹åŒ– - æé«˜è®­ç»ƒç¨³å®šæ€§"""
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    nn.init.constant_(m.weight, 1)
                    nn.init.constant_(m.bias, 0)
                elif isinstance(m, nn.Linear):
                    nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)
        
        def forward(self, x):
            """å‰å‘ä¼ æ’­"""
            # ç‰¹å¾æå–
            x = self.conv_block1(x)  # [B, 64, 112, 112]
            x = self.conv_block2(x)  # [B, 128, 56, 56]
            x = self.conv_block3(x)  # [B, 256, 28, 28]
            x = self.conv_block4(x)  # [B, 512, 14, 14]
            
            # å…¨å±€å¹³å‡æ± åŒ–
            x = self.global_avg_pool(x)  # [B, 512, 1, 1]
            x = x.view(x.size(0), -1)    # [B, 512]
            
            # åˆ†ç±»
            x = self.classifier(x)       # [B, 100]
            
            return x

    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = FruitsCNN(num_classes=num_classes).to(device)

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\nğŸ—ï¸ æ¨¡å‹æ¶æ„:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB")

    # ========================================================================
    # ç¬¬å…«æ­¥ï¼šå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ - å¤šåˆ†ç±»ä¼˜åŒ–ç­–ç•¥ï¼
    # ========================================================================

    """
    ğŸ¯ å¤šåˆ†ç±»è®­ç»ƒç­–ç•¥è¯¦è§£ï¼š
    
    1. æŸå¤±å‡½æ•°é€‰æ‹©ï¼š
       - CrossEntropyLoss: æ ‡å‡†å¤šåˆ†ç±»æŸå¤±
       - ç±»åˆ«æƒé‡: å¤„ç†æ•°æ®ä¸å¹³è¡¡
    
    2. ä¼˜åŒ–å™¨é€‰æ‹©ï¼š
       - Adam: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆå¤æ‚æ¨¡å‹
       - å­¦ä¹ ç‡è°ƒåº¦: è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´
    
    3. æ­£åˆ™åŒ–æŠ€æœ¯ï¼š
       - Weight Decay: L2æ­£åˆ™åŒ–
       - Dropout: éšæœºå¤±æ´»
       - Batch Normalization: æ‰¹é‡å½’ä¸€åŒ–
    """

    # æŸå¤±å‡½æ•°ï¼ˆå¸¦ç±»åˆ«æƒé‡ï¼‰
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        model.parameters(), 
        lr=0.001,           # åˆå§‹å­¦ä¹ ç‡
        weight_decay=1e-4   # L2æ­£åˆ™åŒ–
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',         # ç›‘æ§éªŒè¯æŸå¤±
        factor=0.5,         # å­¦ä¹ ç‡è¡°å‡å› å­
        patience=5,         # ç­‰å¾…è½®æ•°
        min_lr=1e-6        # æœ€å°å­¦ä¹ ç‡
    )

    print(f"âœ… è®­ç»ƒé…ç½®:")
    print(f"  æŸå¤±å‡½æ•°: CrossEntropyLoss (å¸¦ç±»åˆ«æƒé‡)")
    print(f"  ä¼˜åŒ–å™¨: Adam (lr=0.001, weight_decay=1e-4)")
    print(f"  å­¦ä¹ ç‡è°ƒåº¦: ReduceLROnPlateau")

    # ========================================================================
    # ç¬¬ä¹æ­¥ï¼šè®­ç»ƒå’ŒéªŒè¯å‡½æ•° - å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼
    # ========================================================================

    def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        # è¿›åº¦æ˜¾ç¤º
        batch_count = len(train_loader)
        print_interval = max(1, batch_count // 10)  # æ¯10%æ˜¾ç¤ºä¸€æ¬¡
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            # è¿›åº¦æ˜¾ç¤º
            if (batch_idx + 1) % print_interval == 0:
                progress = (batch_idx + 1) / batch_count * 100
                print(f"  è®­ç»ƒè¿›åº¦: {progress:.1f}% | "
                      f"æŸå¤±: {loss.item():.4f} | "
                      f"å‡†ç¡®ç‡: {100 * correct / total:.2f}%")
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100 * correct / total
        
        return epoch_loss, epoch_acc

    def validate_epoch(model, val_loader, criterion, device):
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                
                output = model(data)
                loss = criterion(output, target)
                
                running_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100 * correct / total
        
        return epoch_loss, epoch_acc

    # ========================================================================
    # ç¬¬åæ­¥ï¼šæ¨¡å‹è®­ç»ƒä¸»å¾ªç¯ - å®Œæ•´è®­ç»ƒæµç¨‹ï¼
    # ========================================================================

    def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 
                   device, num_epochs=50, save_path="models/fruits_cnn_best.pth"):
        """å®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹"""
        
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ (å…±{num_epochs}è½®)...")
        print("=" * 80)
        
        # è®­ç»ƒå†å²è®°å½•
        train_losses = []
        train_accs = []
        val_losses = []
        val_accs = []
        
        # æœ€ä½³æ¨¡å‹è®°å½•
        best_val_acc = 0.0
        best_epoch = 0
        patience_counter = 0
        early_stop_patience = 10
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        start_time = time.time()
        
        for epoch in range(num_epochs):
            epoch_start_time = time.time()
            
            print(f"\nğŸ“… Epoch {epoch+1}/{num_epochs}")
            print("-" * 50)
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = train_epoch(
                model, train_loader, criterion, optimizer, device, epoch
            )
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = validate_epoch(
                model, val_loader, criterion, device
            )
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            # è®¡ç®—ç”¨æ—¶
            epoch_time = time.time() - epoch_start_time
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"  è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"  éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_acc:.2f}%")
            print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"  ç”¨æ—¶: {epoch_time:.1f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_epoch = epoch + 1
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_acc': best_val_acc,
                    'class_names': class_names,
                    'class_to_idx': class_to_idx,
                    'num_classes': num_classes
                }, save_path)
                
                print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            else:
                patience_counter += 1
                print(f"â³ éªŒè¯å‡†ç¡®ç‡æœªæå‡ ({patience_counter}/{early_stop_patience})")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= early_stop_patience:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (Epoch {best_epoch})")
                break
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"  æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (Epoch {best_epoch})")
        print(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {save_path}")
        
        return {
            'train_losses': train_losses,
            'train_accs': train_accs,
            'val_losses': val_losses,
            'val_accs': val_accs,
            'best_val_acc': best_val_acc,
            'best_epoch': best_epoch
        }

    # ========================================================================
    # ç¬¬åä¸€æ­¥ï¼šæ¨¡å‹è¯„ä¼°å’Œæµ‹è¯•
    # ========================================================================

    def evaluate_model(model, test_loader, class_names, device):
        """è¯¦ç»†çš„æ¨¡å‹è¯„ä¼°"""
        print("\nğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        model.eval()
        correct = 0
        total = 0
        class_correct = list(0. for i in range(len(class_names)))
        class_total = list(0. for i in range(len(class_names)))
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                _, predicted = torch.max(output, 1)
                
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
                # è®°å½•æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
                c = (predicted == target).squeeze()
                for i in range(target.size(0)):
                    label = target[i]
                    class_correct[label] += c[i].item()
                    class_total[label] += 1
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        # æ€»ä½“å‡†ç¡®ç‡
        overall_acc = 100 * correct / total
        print(f"ğŸ“Š æ€»ä½“æµ‹è¯•å‡†ç¡®ç‡: {overall_acc:.2f}%")
        
        # å„ç±»åˆ«å‡†ç¡®ç‡
        print(f"\nğŸ“‹ å„ç±»åˆ«å‡†ç¡®ç‡ (å‰20ä¸ªç±»åˆ«):")
        for i in range(min(20, len(class_names))):
            if class_total[i] > 0:
                acc = 100 * class_correct[i] / class_total[i]
                print(f"  {class_names[i]}: {acc:.1f}% ({int(class_correct[i])}/{int(class_total[i])})")
        
        return overall_acc, all_predictions, all_targets

    # ========================================================================
    # ç¬¬åäºŒæ­¥ï¼šå•å¼ å›¾ç‰‡é¢„æµ‹å‡½æ•°
    # ========================================================================

    def predict_single_image(model, image_path, class_names, device, transform):
        """é¢„æµ‹å•å¼ å›¾ç‰‡"""
        model.eval()
        
        # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
        image = Image.open(image_path).convert('RGB')
        image_tensor = transform(image).unsqueeze(0).to(device)
        
        with torch.no_grad():
            output = model(image_tensor)
            probabilities = F.softmax(output, dim=1)
            confidence, predicted = torch.max(probabilities, 1)
            
            # è·å–top-5é¢„æµ‹
            top5_prob, top5_idx = torch.topk(probabilities, 5, dim=1)
            
        result = {
            'predicted_class': class_names[predicted.item()],
            'confidence': confidence.item(),
            'top5_predictions': [
                (class_names[idx.item()], prob.item()) 
                for idx, prob in zip(top5_idx[0], top5_prob[0])
            ]
        }
        
        return result

    # ========================================================================
    # ç¬¬åä¸‰æ­¥ï¼šå¯è§†åŒ–å‡½æ•°
    # ========================================================================

    def plot_training_history(history, save_path="results/training_history.png"):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # æŸå¤±æ›²çº¿
        ax1.plot(history['train_losses'], label='è®­ç»ƒæŸå¤±', color='blue')
        ax1.plot(history['val_losses'], label='éªŒè¯æŸå¤±', color='red')
        ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('æŸå¤±')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(history['train_accs'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
        ax2.plot(history['val_accs'], label='éªŒè¯å‡†ç¡®ç‡', color='red')
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        save_dir = Path(save_path).parent
        save_dir.mkdir(parents=True, exist_ok=True)
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“ˆ è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜: {save_path}")

    def visualize_predictions(model, test_loader, class_names, device, num_images=8):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        model.eval()
        
        # è·å–ä¸€æ‰¹æµ‹è¯•æ•°æ®
        data_iter = iter(test_loader)
        images, labels = next(data_iter)
        images, labels = images.to(device), labels.to(device)
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            probabilities = F.softmax(outputs, dim=1)
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        axes = axes.ravel()
        
        for i in range(min(num_images, len(images))):
            # åå½’ä¸€åŒ–å›¾ç‰‡
            img = images[i].cpu()
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            img = img * std + mean
            img = torch.clamp(img, 0, 1)
            
            # æ˜¾ç¤ºå›¾ç‰‡
            axes[i].imshow(img.permute(1, 2, 0))
            
            # æ ‡é¢˜ä¿¡æ¯
            true_label = class_names[labels[i]]
            pred_label = class_names[predicted[i]]
            confidence = probabilities[i][predicted[i]].item()
            
            color = 'green' if predicted[i] == labels[i] else 'red'
            axes[i].set_title(f'çœŸå®: {true_label}\né¢„æµ‹: {pred_label}\nç½®ä¿¡åº¦: {confidence:.2f}', 
                            color=color, fontsize=10)
            axes[i].axis('off')
        
        plt.tight_layout()
        plt.show()

    # ========================================================================
    # ç¬¬åå››æ­¥ï¼šä¸»è®­ç»ƒæµç¨‹
    # ========================================================================

    print("\n" + "="*80)
    print("ğŸ æ°´æœåˆ†ç±»CNNè®­ç»ƒå¼€å§‹!")
    print("="*80)

    # å¼€å§‹è®­ç»ƒ
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        num_epochs=30,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
        save_path="models/fruits_cnn_best.pth"
    )

    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(history)

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    print("\nğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    checkpoint = torch.load("models/fruits_cnn_best.pth", map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])

    # æ¨¡å‹è¯„ä¼°
    test_acc, predictions, targets = evaluate_model(model, test_loader, class_names, device)

    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    print("\nğŸ–¼ï¸ å¯è§†åŒ–é¢„æµ‹ç»“æœ...")
    visualize_predictions(model, test_loader, class_names, device)

    # ========================================================================
    # ç¬¬åäº”æ­¥ï¼šä½¿ç”¨ç¤ºä¾‹å’Œæ€»ç»“
    # ========================================================================

    print("\n" + "="*80)
    print("ğŸ‰ æ°´æœåˆ†ç±»CNNè®­ç»ƒå®Œæˆ!")
    print("="*80)

    print(f"""
    ğŸ“Š è®­ç»ƒæ€»ç»“:
    âœ… æ•°æ®é›†: {num_classes}ç§æ°´æœåˆ†ç±»
    âœ… è®­ç»ƒæ ·æœ¬: {len(train_dataset)}å¼ 
    âœ… éªŒè¯æ ·æœ¬: {len(val_dataset)}å¼ 
    âœ… æµ‹è¯•æ ·æœ¬: {len(test_dataset)}å¼ 
    âœ… æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history['best_val_acc']:.2f}%
    âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%
    âœ… æ¨¡å‹å‚æ•°é‡: {total_params:,}
    
    ğŸ”§ ä½¿ç”¨æ–¹æ³•:
    1. è®­ç»ƒå¥½çš„æ¨¡å‹å·²ä¿å­˜åˆ°: models/fruits_cnn_best.pth
    2. ä½¿ç”¨ predict_single_image() å‡½æ•°é¢„æµ‹æ–°å›¾ç‰‡
    3. ä½¿ç”¨ evaluate_model() å‡½æ•°è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    ğŸ’¡ æ”¹è¿›å»ºè®®:
    1. æ•°æ®å¢å¼º: å°è¯•æ›´å¤šå¢å¼ºæŠ€æœ¯ (MixUp, CutMixç­‰)
    2. æ¨¡å‹æ¶æ„: å°è¯•ResNetã€EfficientNetç­‰å…ˆè¿›æ¶æ„
    3. è¿ç§»å­¦ä¹ : ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åŠ é€Ÿè®­ç»ƒ
    4. é›†æˆå­¦ä¹ : ç»„åˆå¤šä¸ªæ¨¡å‹æå‡æ€§èƒ½
    5. è¶…å‚æ•°ä¼˜åŒ–: ä½¿ç”¨ç½‘æ ¼æœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–
    
    ğŸŒŸ è¿™ä¸ªé¡¹ç›®å±•ç¤ºäº†CNNåœ¨å®é™…å¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„å®Œæ•´åº”ç”¨æµç¨‹ï¼š
    - æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
    - ç±»åˆ«ä¸å¹³è¡¡å¤„ç†
    - æ·±åº¦CNNæ¶æ„è®¾è®¡
    - è®­ç»ƒç­–ç•¥ä¼˜åŒ–
    - æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–
    """)

    # ä¿å­˜ç±»åˆ«æ˜ å°„
    class_mapping = {
        'class_names': class_names,
        'class_to_idx': class_to_idx,
        'num_classes': num_classes
    }
    
    with open('models/fruits_class_mapping.json', 'w', encoding='utf-8') as f:
        json.dump(class_mapping, f, ensure_ascii=False, indent=2)
    
    print("âœ… ç±»åˆ«æ˜ å°„å·²ä¿å­˜åˆ°: models/fruits_class_mapping.json")