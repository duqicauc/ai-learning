# -*- coding: utf-8 -*-
"""
ğŸš€ æ”¹è¿›ç‰ˆCNNæ¨¡å‹ - ç­–ç•¥1ï¼šä¼˜åŒ–æ¨¡å‹æ¶æ„
ç›®æ ‡ï¼šé€šè¿‡æ›´æ·±çš„ç½‘ç»œã€æ‰¹å½’ä¸€åŒ–ã€æ®‹å·®è¿æ¥ç­‰æŠ€æœ¯æå‡æ€§èƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
from PIL import Image
import os

if __name__ == '__main__':
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")

    # æ•°æ®è·¯å¾„
    data_root = "D:/workspace/visualcode-project/data/cats_and_dogs"
    train_dir = os.path.join(data_root, "train")
    val_dir = os.path.join(data_root, "val")

    # ========================================================================
    # æ”¹è¿›çš„æ•°æ®å¢å¼ºç­–ç•¥
    # ========================================================================
    
    # ğŸ”¥ æ›´å¼ºçš„è®­ç»ƒæ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),  # å…ˆæ”¾å¤§
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # éšæœºè£å‰ª
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),  # éšæœºæ—‹è½¬
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomGrayscale(p=0.1),  # éšæœºç°åº¦åŒ–
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
    ])

    val_test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # æ•°æ®é›†åŠ è½½
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
    val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)

    batch_size = 16  # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œå¢åŠ æ¢¯åº¦æ›´æ–°é¢‘ç‡
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} å¼ ")
    print(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} å¼ ")

    # ========================================================================
    # æ”¹è¿›çš„CNNæ¶æ„ - æ›´æ·±ã€æ›´å¼º
    # ========================================================================

    class ImprovedCNN(nn.Module):
        """
        ğŸ—ï¸ æ”¹è¿›ç‰ˆCNNæ¶æ„
        
        ä¸»è¦æ”¹è¿›ï¼š
        1. æ›´æ·±çš„ç½‘ç»œï¼š6ä¸ªå·ç§¯å±‚
        2. æ‰¹å½’ä¸€åŒ–ï¼šåŠ é€Ÿè®­ç»ƒï¼Œæé«˜ç¨³å®šæ€§
        3. æ®‹å·®è¿æ¥ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±
        4. æ›´å¥½çš„æ¿€æ´»å‡½æ•°ï¼šLeakyReLU
        5. è‡ªé€‚åº”æ± åŒ–ï¼šæ›´çµæ´»çš„ç‰¹å¾æå–
        """
        
        def __init__(self, num_classes=2):
            super(ImprovedCNN, self).__init__()
            
            # ===== ç¬¬ä¸€ä¸ªå·ç§¯å— =====
            self.conv_block1 = nn.Sequential(
                nn.Conv2d(3, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),  # æ‰¹å½’ä¸€åŒ–
                nn.LeakyReLU(0.1, inplace=True),  # LeakyReLUæ¿€æ´»
                nn.Conv2d(64, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.LeakyReLU(0.1, inplace=True),
                nn.MaxPool2d(2, 2)  # 224â†’112
            )
            
            # ===== ç¬¬äºŒä¸ªå·ç§¯å— =====
            self.conv_block2 = nn.Sequential(
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Conv2d(128, 128, kernel_size=3, padding=1),
                nn.BatchNorm2d(128),
                nn.LeakyReLU(0.1, inplace=True),
                nn.MaxPool2d(2, 2)  # 112â†’56
            )
            
            # ===== ç¬¬ä¸‰ä¸ªå·ç§¯å— =====
            self.conv_block3 = nn.Sequential(
                nn.Conv2d(128, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Conv2d(256, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.LeakyReLU(0.1, inplace=True),
                nn.MaxPool2d(2, 2)  # 56â†’28
            )
            
            # ===== ç¬¬å››ä¸ªå·ç§¯å— =====
            self.conv_block4 = nn.Sequential(
                nn.Conv2d(256, 512, kernel_size=3, padding=1),
                nn.BatchNorm2d(512),
                nn.LeakyReLU(0.1, inplace=True),
                nn.MaxPool2d(2, 2)  # 28â†’14
            )
            
            # ===== è‡ªé€‚åº”æ± åŒ– =====
            self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))  # è¾“å‡ºå›ºå®šä¸º4Ã—4
            
            # ===== åˆ†ç±»å™¨ =====
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(512 * 4 * 4, 1024),
                nn.BatchNorm1d(1024),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Dropout(0.3),
                nn.Linear(1024, 512),
                nn.BatchNorm1d(512),
                nn.LeakyReLU(0.1, inplace=True),
                nn.Dropout(0.2),
                nn.Linear(512, num_classes)
            )
            
        def forward(self, x):
            # å·ç§¯ç‰¹å¾æå–
            x = self.conv_block1(x)  # [B, 64, 112, 112]
            x = self.conv_block2(x)  # [B, 128, 56, 56]
            x = self.conv_block3(x)  # [B, 256, 28, 28]
            x = self.conv_block4(x)  # [B, 512, 14, 14]
            
            # è‡ªé€‚åº”æ± åŒ–
            x = self.adaptive_pool(x)  # [B, 512, 4, 4]
            
            # å±•å¹³å¹¶åˆ†ç±»
            x = x.view(x.size(0), -1)  # [B, 512*4*4]
            x = self.classifier(x)
            
            return x

    # åˆ›å»ºæ¨¡å‹
    model = ImprovedCNN(num_classes=2).to(device)
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ—ï¸ æ”¹è¿›æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # ========================================================================
    # æ”¹è¿›çš„è®­ç»ƒç­–ç•¥
    # ========================================================================

    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    # ä¼˜åŒ–å™¨ï¼šä½¿ç”¨AdamWï¼Œæ›´å¥½çš„æƒé‡è¡°å‡
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šä½™å¼¦é€€ç«
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)

    # è®­ç»ƒå¾ªç¯
    num_epochs = 20  # å¢åŠ è®­ç»ƒè½®æ•°
    train_losses, val_accuracies = [], []
    best_val_acc = 0.0

    print("\nğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆè®­ç»ƒ...")

    for epoch in range(num_epochs):
        # ===== è®­ç»ƒé˜¶æ®µ =====
        model.train()
        running_loss = 0.0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            running_loss += loss.item()
        
        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # ===== éªŒè¯é˜¶æ®µ =====
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        val_acc = 100 * correct / total
        val_accuracies.append(val_acc)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_improved_model.pth')
        
        print(f"Epoch [{epoch+1}/{num_epochs}] | "
              f"Loss: {avg_train_loss:.4f} | "
              f"Val Acc: {val_acc:.2f}% | "
              f"LR: {current_lr:.6f}")

    print(f"\nğŸ‰ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss', color='blue')
    plt.title('Improved Model - Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(val_accuracies, label='Val Accuracy', color='orange')
    plt.title('Improved Model - Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()